{
  
    
        "post0": {
            "title": "Equilibrium Binding of Transcription Factors",
            "content": "# imports import string from collections import defaultdict import itertools from random import choice, random from scipy.stats import bernoulli, norm, lognorm, expon as exponential from scipy.stats import rv_discrete, rv_continuous import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt from matplotlib.patches import Rectangle, Polygon from matplotlib.colors import colorConverter, TABLEAU_COLORS # colorConverter.to_rgba(&#39;mediumseagreen&#39;, alpha=.5) import plotly.graph_objects as go %config InlineBackend.figure_format = &#39;retina&#39; %matplotlib inline from IPython.display import set_matplotlib_formats set_matplotlib_formats(&#39;svg&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 plt.rcParams[&#39;agg.path.chunksize&#39;] = 10000 plt.rcParams[&#39;animation.html&#39;] = &#39;jshtml&#39; plt.rcParams[&#39;hatch.linewidth&#39;] = 0.3 from IPython.display import display, HTML # brew install ghostscript # brew install pdf2svg # pip install seqlogo import seqlogo import requests from xml.etree import ElementTree ln = np.log exp = np.exp . # distribution plotting utilities def is_discrete(dist): if hasattr(dist, &#39;dist&#39;): return isinstance(dist.dist, rv_discrete) else: return isinstance(dist, rv_discrete) def is_continuous(dist): if hasattr(dist, &#39;dist&#39;): return isinstance(dist.dist, rv_continuous) else: return isinstance(dist, rv_continuous) def plot_distrib(distrib, title=None): fig, ax = plt.subplots(1, 1) if is_continuous(distrib): x = np.linspace(distrib.ppf(0.001), distrib.ppf(0.999), 1000) ax.plot(x, distrib.pdf(x), &#39;k-&#39;, lw=0.4) elif is_discrete(distrib): x = np.arange(distrib.ppf(0.01), distrib.ppf(0.99)) ax.plot(x, distrib.pmf(x), &#39;bo&#39;, ms=2, lw=0.4) r = distrib.rvs(size=1000) ax.hist(r, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, bins=200) if title: ax.set_title(title) fig_style_2(ax) return ax def fig_style_2(ax): for side in [&quot;right&quot;,&quot;top&quot;,&quot;left&quot;]: ax.spines[side].set_visible(False) ax.get_yaxis().set_visible(False) . Single TF . $$ newcommand{ kon}{k_{ mathrm{on}}} newcommand{ koff}{k_{ mathrm{off}}}$$There exist two mathematical formalisms to describe the probabilities of molecular configurations at equilibrium: thermo and kinetics. In the kinetics formalism, the system transits between configurational states according to rate parameters contained in differential equations, which can describe not just the system&#39;s equilibrium but also its trajectory towards it, or the kinetics of non-equilibrium systems. In the thermodynamics/statistical mechanics formalism, we posit that a system will occupy configurations with lower energy, and use the Boltzmann distribution to estimate the proportion of time the system spends in each state, at equilibrium. The thermodynamics formalism is limited in that it only describes equilibrium, but it achieves this with fewer, more intuitive parameters. . We&#39;ll derive an expression for the probability of a single TFBS&#39; occupancy with both formalisms, but proceed with the thermodynamic description for more elaborate configurations. It will become clear why that is preferable. . Kinetics . Most derivations of the probability of single TFBS occupancy at equilibrium employ a kinetics formalism, so we&#39;ll walk through that first, and then explore the analog in the thermodynamics description. In the kinetics description, the parameters are rates. . $$ mathrm{TF} + mathrm{TFBS} underset{ koff}{ overset{ kon}{ rightleftarrows}} mathrm{TF colon TFBS} $$ . The natural rates are the rate of TF binding $ kon$ and unbinding $ koff$. Equilibrium is reached when binding and unbinding are balanced: . $$ frac{d[ mathrm{TF colon TFBS}]}{dt} = k_{ mathrm{on}}[ mathrm{TF}][ mathrm{TFBS}] - k_{ mathrm{off}}[ mathrm{TF colon TFBS}] = 0 text{ at equilibrium}$$ $$k_{ mathrm{on}}[ mathrm{TF}]_{ mathrm{eq}}[ mathrm{TFBS}]_{ mathrm{eq}} = k_{ mathrm{off}}[ mathrm{TF colon TFBS}]_{ mathrm{eq}}$$ $$ text{(dropping eq subscript) }[ mathrm{TF colon TFBS}] = frac{k_{ mathrm{on}}[ mathrm{TF}][ mathrm{TFBS}]}{k_{ mathrm{off}}} = frac{[ mathrm{TF}][ mathrm{TFBS}]}{k_{d}}$$ . where $k_{d} = frac{ koff}{ kon}$ is called the dissociation constant (or equilibrium constant). We&#39;d like to determine the probability of finding the TFBS occupied, i.e. the fraction of time it spends in the bound state. That fraction is $ frac{[ mathrm{bound}]}{([ mathrm{unbound}] + [ mathrm{bound}])} = frac{[ mathrm{TF colon TFBS}]}{([ mathrm{TFBS}] + [ mathrm{TF colon TFBS}])}$. Define the denominator as $[ mathrm{TFBS}]_{0} = [ mathrm{TFBS}] + [ mathrm{TF colon TFBS}]$ so that $[ mathrm{TFBS}] = [ mathrm{TFBS}]_{0} - [ mathrm{TF colon TFBS}]$ and substitute: . $$[ mathrm{TF colon TFBS}] = frac{[ mathrm{TF}]([ mathrm{TFBS}]_{0} - [ mathrm{TF colon TFBS}])}{k_{d}}$$ $$[ mathrm{TF colon TFBS}](k_d + [ mathrm{TF}]) = [ mathrm{TF}][ mathrm{TFBS}]_{0}$$ $$ frac{[ mathrm{TF colon TFBS}]}{[ mathrm{TFBS}]_{0}} = frac{[ mathrm{TF}]}{k_d + [ mathrm{TF}]}$$ . Note: We could also ask for this expression in terms of $[ mathrm{TF}]_0 = [ mathrm{TF}] + [ mathrm{TF colon TFBS}]$ however, since we&#8217;re considering a single TFBS, $[ mathrm{TF colon TFBS}]$ is at most 1, and so $[ mathrm{TF}]_0 approx [ mathrm{TF}]$. In instances of ligand-receptor binding in which that approximation cannot be made, the fraction bound is a messy quadratic. Derivation here. . Thermodynamics . In the thermodynamics description, the parameters are Gibbs free energies $ Delta G$. Let&#39;s follow the derivation from Physical Biology of the Cell (pp. 242) and consider the number microstates underlying each of the of bound and unbound macrostates, and their energies. . In order to count microstates, we imagine distributing $L$ TF molecules across a space-filling lattice with $ Omega$ sites. The energy of a TF in solution is $ varepsilon_{ mathrm{solution}}$ and the energy of a bound TF is $ varepsilon_{ mathrm{bound}}$. $ beta$ is the constant $1/k_b T$ where $k_b$ is Boltzmann&#39;s constant and $T$ is the temperature. . &lt;!-- . State | Energy | Multiplicity | Weight | . | $A cdot A_s$ | $ frac{ Omega!}{( Omega - A)!A!} approx frac{ Omega^{A}}{A!}$ | $ frac{ Omega^{A}}{A!} cdot e^{- beta left[ A cdot A_s right]}$ | . | $(A - 1) A_s + A_b$ | $ frac{ Omega!}{( Omega - (A - 1))!(A-1)!B!} approx frac{ Omega^{A-1}}{(A-1)!}$ | $ frac{ Omega^{A-1}}{(A-1)!} cdot e^{- beta left[ (A - 1) A_s + A_b right]}$ | . --&gt; . In our case, the number of microstates in the unbound macrostate is $ frac{ Omega !}{L!( Omega -L)!} approx frac{ Omega^L}{L!}$ and they each have energy $L cdot varepsilon_s$. The number of microstates in the bound macrostate is $ frac{ Omega !}{(L-1)!( Omega -(L+1))} approx frac{ Omega^{(L-1)}}{(L-1)!}$ and they each have energy $(L-1) varepsilon_s + varepsilon_b$. . The Boltzmann distribution describes the probability of a microstate as a function of its energy: $p(E_i) = e^{- beta E_i}/Z$ where $Z$ is the &quot;partition function&quot; or simply $ sum_i e^{- beta E_i}$ the sum of the weights of the microstates, which normalizes the distribution. In our case: . $$Z(L, Omega)= left( colorbox{LightCyan}{$ frac{ Omega^L}{L!} e^{- beta L varepsilon_s}$} right) + left( colorbox{Seashell}{$ frac{ Omega^{(L-1)}}{(L-1)!} e^{- beta [(L-1) varepsilon_s + varepsilon_b]}$} right)$$ . With that expression in hand, we can express the probability of the bound macrostate, $p_b$: . $$p_b= frac{ colorbox{Seashell}{$ frac{ Omega^{(L-1)}}{(L-1)!} e^{- beta [(L-1) varepsilon_s + varepsilon_b]}$}}{ colorbox{LightCyan}{$ frac{ Omega^L}{L!} e^{- beta L varepsilon_s}$} + colorbox{Seashell}{$ frac{ Omega^{(L-1)}}{(L-1)!} e^{- beta [(L-1) varepsilon_s + varepsilon_b]}$}} cdot color{DarkRed} frac{ frac{ Omega^L}{L!}e^{ beta L varepsilon_s}}{ frac{ Omega^L}{L!}e^{ beta L varepsilon_s}} color{black} = frac{(L/ Omega)e^{- beta Delta varepsilon}}{1+(L/ Omega)e^{- beta Delta varepsilon}} $$ . Where we have defined $ Delta varepsilon = varepsilon_b - varepsilon_s$. $L/ Omega$ is really just a dimensionless TF concentration, which we&#39;ll hand-wave as being equivalent to $[ mathrm{TF}]$, which leaves us with an expression suspiciously similar to the one we derived from the kinetics formalism: . $$p_b = frac{[ mathrm{TF}]e^{- beta Delta varepsilon}}{1+[ mathrm{TF}]e^{- beta Delta varepsilon}} cdot color{DarkRed} frac{e^{ beta Delta varepsilon}}{e^{ beta Delta varepsilon}} color{black} = frac{[ mathrm{TF}]}{e^{ beta Delta varepsilon}+[ mathrm{TF}]}$$ . From which we recapitulate an important correspondence between kinetics and thermodynamics at equilibrium: $ k_d = e^{ beta Delta varepsilon} = e^{ Delta varepsilon / k_bT} $ more commonly written for different units as $k = e^{- Delta G / RT}$. . The takeaway is that both the kinetics and thermodynamics formalisms produce an equivalent expression for the probabilities of each of the bound and unbound configurations, parameterized respectively by $k_d$ and $ Delta G$. . References: . Physical Biology of the Cell | Thermodynamics of Biological Processes | Statistical Mechanics of Binding | . Sample Values . In order to compute probabilities like $p_b$, we need concrete TF concentrations $[ mathrm{TF}]$ and binding affinities (either $k_d$ or $ Delta G$). What are typical intranuclear TF concentrations and binding affinities? . Concentrations . A typical human cell nucleus has diameter 6Î¼m, so spherical volume = $ frac{4}{3} pi r^3 = frac{4}{3} pi (3 mathrm{ mu m})^3 approx 113 mathrm{ mu m}^3$. (MBoC.) . A typical expressed TF has a per-cell copy number range from $10^3$ - $10^6$. (BioNumbers) . copy_number_range = [1e3, 1e6] nuclear_volume = 113 N_A = 6.02214076e23 def copy_number_to_concentration(copy_number): return (copy_number / N_A) / (nuclear_volume * (1e3 / 1e18)) lower_end_molar = copy_number_to_concentration(copy_number_range[0]) upper_end_molar = copy_number_to_concentration(copy_number_range[1]) lower_end_nanomolar = lower_end_molar / 1e-9 upper_end_nanomolar = upper_end_molar / 1e-9 print(&#39;If TF copy numbers range from 1,000-1,000,000, then TF concentrations range from&#39;, str(round(lower_end_nanomolar))+&#39;nM&#39;, &#39;to&#39;, str(round(upper_end_nanomolar))+&#39;nM&#39;) . If TF copy numbers range from 1,000-1,000,000, then TF concentrations range from 15nM to 14695nM . We might also like a distribution over this range. Let&#39;s posit a lognormal, where $10^3$ and $10^6$ are the 3Ï from the mean, which is $10^{4.5}$. Then $ sigma = 10^{0.5}$ . # define a distribution over TF copy numbers # Note: the lognormal is defined with base e, so we need to take some natural logs on our base 10 expression. TF_copy_number_distrib = lognorm(scale=10**4.5, s=np.log(10**0.5)) ax = plot_distrib(TF_copy_number_distrib, title=&#39;Hypothetical expressed TF copy number distribution&#39;) ax.set_xlim(left=0, right=5e5) ax.set_xlabel(&#39;TF protein copy number / cell&#39;) ax.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), &#39;,&#39;))) def TF_nanomolar_concentrations_sample(TFs): return dict(zip(TFs, (copy_number_to_concentration(TF_copy_number_distrib.rvs(len(TFs)))*1e9).astype(int))) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-13T14:22:26.948803 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ Affinities . What are typical TF ÎG&#39;s of binding? How about the $ koff$ rates and half lives? . We can use the prior knowledge that dissociation constants should be in the nanomolar regime (BioNumbers). | We can use the relation that $ Delta G = -k_b T cdot ln(k_d)$ (Plugging in 310Â°K (human body temp) and the Boltzmann constant $k_b$ in kcal/Mol) | We use the approximation that $ kon$ is ~$10^5 / $ Molar $ times $ sec (Wittrup) | . T = 310 k_b = 3.297623483e-24 * 1e-3 ## cal/K * kcal/cal kbT = k_b*T*N_A kbT ## in 1/Mol -- an unusual format k_on = 1e5 def nanomolar_kd_from_kcal_ÎG(ÎG): return exp(-ÎG/kbT) * 1e9 def kcal_ÎG_from_nanomolar_kd(K_d): return -kbT*ln(K_d*1e-9) def k_off_from_nanomolar_kd(k_d): return (k_d*1e-9) * k_on def half_life_from_kd(k_d): return ln(2) / ((k_d*1e-9) * k_on) . # compute statistics from kds nanomolar_kds = pd.Series([1, 10, 100, 1000]) affinity_grid = pd.DataFrame() affinity_grid[&#39;$k_d$&#39;] = nanomolar_kds affinity_grid[&#39;$ Delta G$&#39;] = nanomolar_kds.apply(kcal_ÎG_from_nanomolar_kd) affinity_grid[&#39;$ kon$&#39;] = &#39;1e5 / (M * s)&#39; affinity_grid[&#39;$ koff$&#39;] = nanomolar_kds.apply(k_off_from_nanomolar_kd) affinity_grid[&#39;$t_{1/2}$&#39;] = pd.to_timedelta(nanomolar_kds.apply(half_life_from_kd), unit=&#39;s&#39;) affinity_grid = affinity_grid.set_index(&#39;$k_d$&#39;) affinity_grid . $ Delta G$ $ kon$ $ koff$ $t_{1/2}$ . $k_d$ . 1 12.757685 | 1e5 / (M * s) | 0.0001 | 0 days 01:55:31.471805599 | . 10 11.340164 | 1e5 / (M * s) | 0.0010 | 0 days 00:11:33.147180560 | . 100 9.922644 | 1e5 / (M * s) | 0.0100 | 0 days 00:01:09.314718056 | . 1000 8.505123 | 1e5 / (M * s) | 0.1000 | 0 days 00:00:06.931471806 | . We learn that an order of magnitude residence time difference results from just 1.4 extra kcal/Mol, and that TF half lives range from about 5s to about 2h. Let&#39;s once again posit a distribution of affinities to sample from (defined on $k_d$): . # define a distribution over TF Kd&#39;s / ÎG&#39;s TF_affinity_min = 6 ## define exponential distribution in log10 space TF_affinity_spread = 0.5 TF_affinity_distrib = exponential(loc=TF_affinity_min, scale=TF_affinity_spread) ax = plot_distrib(TF_affinity_distrib, title=&quot;Hypothetical TF $K_d$ distribution&quot;) ax.set_xlim(left=5.9) ax.set_xlabel(&#39;$k_d$&#39;) plt.xticks([6,7,8,9], [&#39;1000Î¼m&#39;, &#39;100nm&#39;, &#39;10nm&#39;, &#39;1nm&#39;]) def TF_Kd_sample(n=1): return 10**(-TF_affinity_distrib.rvs(n)) def TF_ÎG_sample(n=1): return kcal_ÎG_from_nanomolar_kd(10**(-TF_affinity_distrib.rvs(n)+9)) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-13T14:22:29.168471 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ With those concrete TF concentrations and dissociation constants, we can finally plot our function $p_b = frac{[ mathrm{TF}]}{e^{ beta Delta varepsilon}+[ mathrm{TF}]}$. . @np.vectorize def fraction_bound(TF, ÎG): &#39;&#39;&#39;TF in nanomolar&#39;&#39;&#39; return TF / (TF + nanomolar_kd_from_kcal_ÎG(ÎG)) . # plot fraction bound as a function of concentration and binding energy TF_concentration_array = np.logspace(1, 5) ÎG_array = np.logspace(*np.log10([8, 13])) TF_concs_matrix, ÎG_matrix = np.meshgrid(TF_concentration_array, ÎG_array) z_data = pd.DataFrame(fraction_bound(TF_concs_matrix, ÎG_matrix), index=ÎG_array, columns=TF_concentration_array).rename_axis(&#39;ÎG&#39;).T.rename_axis(&#39;[TF]&#39;) fig = go.Figure(data=[go.Surface(x=TF_concentration_array.astype(int).astype(str), y=ÎG_array.round(1).astype(str), z=z_data.values)]) fig.update_layout( title=&#39;&#39;, autosize=False, width=700, margin=dict(r=20, l=10, b=10, t=10), scene = dict( xaxis_title=&#39;[TF]&#39;, yaxis_title=&#39;ÎG&#39;, zaxis_title=&#39;Pb&#39;), scene_camera = dict(eye=dict(x=-1, y=-1.8, z=1.25))) fig.update_traces(showscale=False) # config = dict({&#39;scrollZoom&#39;: False}) # fig.show(config = config) # display(fig) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;, include_mathjax=False)) . . . (Note that both $[ mathrm{TF}]$ and $k_d$ are plotted in log space, but $p_b$ is linear.) . Multiple TFs: Direct Cooperativity &amp; Competition . Suppose now that two TFs bind adjacent segments of DNA in such a way that the binding of either facilitates (or hinders) the binding of the other. We call this direct cooperativity (and competition). We&#39;ll focus first on cooperativity. . Cooperativity . As before, the statistical mechanics formalism entails enumerating the configurations, their multiplicities, and their energies. We&#39;ll call the TFs A and B. We&#39;ll denote their counts as $A$ and $B$. The energy of a TF in solution will once more be $A_s$ and bound to its cognate TFBS $A_b$. The energy of cooperativity will be $C_{AB}$. . State | Energy | Multiplicity | Weight | . | $A cdot A_s + B cdot B_s$ | $ frac{ Omega!}{( Omega - A - B)!A!B!} approx frac{ Omega^{A+B}}{A!B!}$ | $ frac{ Omega^{A+B}}{A!B!} cdot e^{- beta left[ A cdot A_s + B cdot B_s right]}$ | . | $(A - 1) A_s + A_b + B cdot B_s$ | $ frac{ Omega!}{( Omega - (A - 1) - B)!(A-1)!B!} approx frac{ Omega^{A+B-1}}{(A-1)!B!}$ | $ frac{ Omega^{A+B-1}}{(A-1)!B!} cdot e^{- beta left[ (A - 1) A_s + A_b + B cdot B_s right]}$ | . | $A cdot A_s + (B - 1) B_s + B_b$ | $ frac{ Omega!}{( Omega - A - (B - 1))!A!(B-1)!} approx frac{ Omega^{A+B-1}}{A!(B-1)!}$ | $ frac{ Omega^{A+B-1}}{A!(B-1)!} cdot e^{- beta left[ A cdot A_s + (B - 1) B_s + B_b right]}$ | . | $(A - 1) A_s + A_b + (B - 1) B_s + B_b + C_{AB}$ | $ frac{ Omega!}{( Omega - (A - 1) - (B-1))!(A-1)!(B-1)!} approx frac{ Omega^{A+B-2}}{(A-1)!(B-1)!}$ | $ frac{ Omega^{A+B-2}}{(A-1)!(B-1)!} cdot e^{- beta left[ (A - 1) A_s + A_b + (B - 1) B_s + B_b + C_{AB} right]}$ | . The partition function is the sum of the weights: . $$ Z = frac{ Omega^{A+B}}{A!B!} cdot e^{- beta left[ A cdot A_s + B cdot B_s right]} + frac{ Omega^{A+B-1}}{(A-1)!B!} cdot e^{- beta left[ (A - 1) A_s + A_b + B cdot B_s right]} + frac{ Omega^{A+B-1}}{A!(B-1)!} cdot e^{- beta left[ A cdot A_s + (B - 1) B_s + B_b right]} + frac{ Omega^{A+B-2}}{(A-1)!(B-1)!} cdot e^{- beta left[ (A - 1) A_s + A_b + (B - 1) B_s + B_b + C_{AB} right]}$$Which we can greatly simplify by multiplying the entire expression by the reciprocal of the &quot;base state&quot; weight, $ color{DarkRed} frac{A!B!}{ Omega^{A+B}} cdot e^{ beta left[ A cdot A_s + B cdot B_s right]}$, normalizing that weight to 1: . $$ Z = 1 + frac{A}{ Omega} cdot e^{- beta left[ A_b-A_s right]} + frac{B}{ Omega} cdot e^{- beta left[ B_b-B_s right]} + frac{A cdot B}{ Omega^2} cdot e^{- beta left[ A_b-A_s+B_b-B_s+C_{AB} right]}$$Taking the definition $[A] = A/ Omega$ and $ Delta G_A = A_b-A_s$ produces: . $$ Z = 1 + [A] e^{- beta left[ Delta G_A right]} + [B] e^{- beta left[ Delta G_B right]} + [A][B] e^{- beta left[ Delta G_A+ Delta G_B+C_{AB} right]}$$Then the probability of any state is just the weight of that state (scaled by the weight of the base state) divided by the partition function expression $Z$. . From the above, we notice the form of the expression for the weight of a configuration of N TFBSs: . $$ p_{ mathrm{config}} = prod_{i in , mathrm{bound ,TBFS}} left( [ mathrm{TF}_{ mathrm{cognate}(i)}] cdot e^{- beta left[ Delta G_i + sum_j c_{ij} right]} right) / Z$$ . For numerical stability, we take the log of the unnormalized probability (that is, the weight) of configurations: . $$ log( tilde{p}_{ mathrm{config}}) = sum_{i in , mathrm{bound ,TBFS}} left( log([ mathrm{TF}_{ mathrm{cognate}(i)}]) - beta left[ Delta G_i + sum_j c_{ij} right] right) $$ . # define log_P_config() Î² = 1/kbT ## kbT was in in 1/Mol def log_P_config(config, TF_conc, TFBSs, cooperativities): logP = 0 for i, tfbs in TFBSs[config.astype(bool)].iterrows(): cooperativity_sum = 0 if i in cooperativities: cooperativity_sum = sum([C_AB for tfbs_j, C_AB in cooperativities[i].items() if config[tfbs_j] == 1]) logP = sum([np.log(TF_conc[tfbs.TF]*1e-9) + Î²*(tfbs.dG + cooperativity_sum)]) ## the sign is flipped here, because our ÎG&#39;s of binding are positive above. return logP . Competition . Incorporating competition into our thermodynamic model is slightly more subtle than cooperativity, because we can imagine two types of competition . Two TFs which may both bind DNA at adjacent sites, causing a free energy penalty due to some unfavorable interaction. | Two TFs with overlapping DNA binding sites, which cannot physically be bound at the same time. | . In the former case, the expression for $p_ mathrm{config}$ we had written before suffices, with values of $C_{AB}$ having both signs to represent cooperativity and competition. Nominally, the latter case also fits this formalism if we allow $C_{AB}$ to reach $- infty$, but that would cause us headaches in the implementation. Instead, the weights of all those configurations which are not physical attainable, due to &quot;strict&quot; competitive binding between TFs vying for overlapping binding sites, are merely omitted from the denominator $Z$. . References: . Transcriptional regulation by the numbers: models | Thermodynamic State Ensemble Models of cis-Regulation | . Sample cooperativity values . In order to compute concrete probabilities of configurations, accounting for cooperativity and competition, we will need concrete energies. We&#39;ll take $C_{AB}$ to be distributed exponentially with a mean at 2.2kcal/Mol. (ForsÃ©n &amp; Linse). . # define a distribution of cooperativities cooperativity_mean_ÎG = 2.2 cooperativity_distrib = exponential(scale=cooperativity_mean_ÎG) ax = plot_distrib(cooperativity_distrib, title=&quot;Hypothetical $C_{AB}$ distribution&quot;) ax.set_xlim(left=-0.5,right=15) ax.set_xlabel(&#39;$C_{AB}$ (kcal/mol)&#39;) def C_AB_sample(n=1): return cooperativity_distrib.rvs(n) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-09T09:56:51.040609 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ # sample that distribution for cooperativities between binding sites def sample_cooperativities(TFBSs): cooperativities = defaultdict(lambda: dict()) for i, tfbs_i in TFBSs.iterrows(): for j, tfbs_j in TFBSs.iterrows(): if i &lt; j: if 7 &lt;= abs(tfbs_i.start - tfbs_j.start) &lt;= 10: cooperativities[i][j] = cooperativities[j][i] = C_AB_sample()[0] elif abs(tfbs_i.start - tfbs_j.start) &lt; 7: cooperativities[i][j] = cooperativities[j][i] = -C_AB_sample()[0] return dict(cooperativities) . Let&#39;s check our derivation (and our implementation) of $p_ mathrm{config}$ by comparing it to our direct computation of $p_b$ from Â§1. Single TF. . # define create_environment() len_TFBS=10 def create_environment(len_DNA=1000, num_TFs=10, num_TFBS=20, len_TFBS=10): # TFBSs is a dataframe with columns &#39;TF_name&#39;, &#39;start&#39;, &#39;dG&#39; TFs = list(string.ascii_uppercase[:num_TFs]) ## TF names are just letters from the alphabet TF_conc = TF_nanomolar_concentrations_sample(TFs) TFBSs = pd.DataFrame([{&#39;TF&#39;: choice(TFs), &#39;start&#39;: int(random()*(len_DNA-len_TFBS)), &#39;dG&#39;: TF_ÎG_sample()[0]} for _ in range(num_TFBS)]).sort_values(by=&#39;start&#39;).reset_index(drop=True) cooperativities = sample_cooperativities(TFBSs) return TFs, TF_conc, TFBSs, cooperativities . # define draw_config() for plotting def draw_config(TFBSs, TF_conc, cooperativities, config=None, len_DNA=1000): if config is None: config = [0]*len(TFBSs) TF_colors = dict(zip(list(TF_conc.keys()), list(TABLEAU_COLORS.values()))) plt.rcParams[&#39;figure.figsize&#39;] = [12, 0.5+np.sqrt(len(TFs))] fig, axs = plt.subplots(ncols=2, sharey=True, gridspec_kw={&#39;width_ratios&#39;: [4, 1]}) genome_track_ax = draw_genome_track(axs[0], config, TFBSs, cooperativities, TF_colors, len_DNA=len_DNA) conc_plot_ax = draw_concentration_plot(axs[1], TF_conc, TF_colors) return genome_track_ax, conc_plot_ax def draw_concentration_plot(conc_plot_ax, TF_conc, TF_colors): conc_plot_ax.barh(range(len(TF_conc.keys())), TF_conc.values(), align=&#39;edge&#39;, color=list(TF_colors.values()), alpha=0.9) for p in conc_plot_ax.patches: conc_plot_ax.annotate(str(p.get_width())+&#39;nm&#39;, (p.get_width() + 10*(p.get_width()&gt;0), p.get_y() * 1.02), fontsize=&#39;x-small&#39;) conc_plot_ax.axes.get_yaxis().set_visible(False) conc_plot_ax.axes.get_xaxis().set_visible(False) conc_plot_ax.set_frame_on(False) return conc_plot_ax def draw_genome_track(genome_track_ax, config, TFBSs, cooperativities, TF_colors, len_DNA=1000): genome_track_ax.set(ylabel=&#39;TFs&#39;, ylim=[-1, len(TF_colors.keys())+1], yticks=range(len(TFs)), yticklabels=TFs, xlabel=&#39;Genome&#39;, xlim=[0, len_DNA]) for i, tfbs in TFBSs.iterrows(): tfbs_scale = np.clip(0.01*np.exp(tfbs.dG-7), 0, 1) genome_track_ax.add_patch(Rectangle((tfbs.start, TFs.index(tfbs.TF)), len_TFBS, 0.8, fc=TF_colors[tfbs.TF], alpha=tfbs_scale)) genome_track_ax.add_patch(Rectangle((tfbs.start, TFs.index(tfbs.TF)), len_TFBS, 0.1*tfbs_scale, fc=TF_colors[tfbs.TF], alpha=1)) genome_track_ax.annotate(str(int(tfbs.dG))+&#39;kcal/Mol&#39;, (tfbs.start, TFs.index(tfbs.TF)-0.3), fontsize=&#39;xx-small&#39;) genome_track_ax.add_patch(Polygon([[tfbs.start+2, TFs.index(tfbs.TF)], [tfbs.start+5,TFs.index(tfbs.TF)+0.8],[tfbs.start+8, TFs.index(tfbs.TF)]], fc=TF_colors[tfbs.TF], alpha=config[i])) cm = matplotlib.cm.ScalarMappable(norm=matplotlib.colors.Normalize(vmin=-3, vmax=3), cmap=matplotlib.cm.PiYG) for i, rest in cooperativities.items(): for j, C_AB in rest.items(): tfbs_i = TFBSs.iloc[i] tfbs_j = TFBSs.iloc[j] xa = tfbs_i.start+(len_TFBS/2) xb = tfbs_j.start+(len_TFBS/2) ya = TFs.index(tfbs_i.TF) yb = TFs.index(tfbs_j.TF) genome_track_ax.plot([xa, (xa+xb)/2, xb], [ya+0.9, max(ya, yb)+1.2, yb+0.9], color=cm.to_rgba(C_AB)) genome_track_ax.grid(axis=&#39;y&#39;, lw=0.1) genome_track_ax.set_frame_on(False) return genome_track_ax . def enumerate_configs(TFBSs): return list(map(np.array, itertools.product([0,1], repeat=len(TFBSs)))) def p_configs(TFBSs, TF_conc, cooperativities): configs = enumerate_configs(TFBSs) weights = [] for config in configs: weights.append(np.exp(log_P_config(config, TFBSs=TFBSs, TF_conc=TF_conc, cooperativities=cooperativities))) return list(zip(configs, np.array(weights) / sum(weights))) . TFs, TF_conc, TFBSs, cooperativities = create_environment(len_DNA=100, num_TFs=1, num_TFBS=1) print(&#39;p_bound (prev method): t&#39;, fraction_bound(TF_conc[&#39;A&#39;], TFBSs.iloc[0].dG)) print(&#39;p_config (new method): t&#39;, p_configs(TFBSs=TFBSs, TF_conc=TF_conc, cooperativities={})) genome_track_ax, conc_plot_ax = draw_config(TFBSs=TFBSs, TF_conc=TF_conc, cooperativities=cooperativities, len_DNA=100) plt.tight_layout() . p_bound (prev method): 0.5517750016034638 p_config (new method): [(array([0]), 0.4482249983965356), (array([1]), 0.5517750016034644)] . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-09T09:58:28.482792 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ With that sanity check, let&#39;s now consider the scenario of two transcription factors with cooperative binding, and compute the probabilities of each of the 4 configurations: . # Create cooperative environment len_DNA = 100 TFs = [&#39;A&#39;, &#39;B&#39;] TF_conc = TF_nanomolar_concentrations_sample(TFs) TFBSs = pd.DataFrame([{&#39;TF&#39;: &#39;A&#39;, &#39;start&#39;: 10, &#39;dG&#39;: TF_ÎG_sample()[0]}, {&#39;TF&#39;: &#39;B&#39;, &#39;start&#39;: 20, &#39;dG&#39;: TF_ÎG_sample()[0]}]) cooperativities = {0: {1: 2}, 1: {0: 2}} TF_colors = dict(zip(list(TF_conc.keys()), list(TABLEAU_COLORS.values()))) plt.rcParams[&#39;figure.figsize&#39;] = [12, 2*(0.8+int(np.sqrt(len(TFs))))] fig, axs = plt.subplots(nrows=2, ncols=2) for ax, (config, p) in zip([ax for row in axs for ax in row], p_configs(TFBSs=TFBSs, TF_conc=TF_conc, cooperativities=cooperativities)): draw_genome_track(ax, config, TFBSs, cooperativities, TF_colors, len_DNA=50) ax.set(ylabel=&#39;&#39;, xlabel=&#39;&#39;) ax.set_title(&#39;$p_ mathrm{config}$: &#39;+str(round(p*100,3))+&#39;%&#39;, y=1.0, pad=-28, loc=&#39;left&#39;, fontsize=10) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-09T09:58:29.678105 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ Low-Affinity binding . In reality, transcription factors binding sites are not discretely present or absent: transcription factors may bind anywhere along the genome, with varying affinity. Let&#39;s introduce mono-nucleotide transcription factor binding models: . id=13165 . with open(f&#39;/Users/alex/Documents/AChroMap/data/raw/ProBound/json/{id}.json&#39;) as json_file: f = json.load(json_file) mononucleotide_model = f[&#39;coefficients&#39;][&#39;bindingModes&#39;][0][&#39;mononucleotide&#39;] pwm = np.reshape(mononucleotide_model, (len(mononucleotide_model)//4,4)) . base_index = f[&#39;modelSettings&#39;][&#39;letterOrder&#39;] . SPI1_best_motif = &#39;&#39;.join([base_index[np.argwhere(row == 0)[0][0]] for row in pwm]) . complement = {&#39;A&#39;:&#39;T&#39;, &#39;T&#39;:&#39;A&#39;, &#39;C&#39;:&#39;G&#39;, &#39;G&#39;:&#39;C&#39;, &#39;a&#39;:&#39;t&#39;, &#39;t&#39;:&#39;a&#39;, &#39;c&#39;:&#39;g&#39;, &#39;g&#39;:&#39;c&#39;} def reverse_complement(dna): list_repr = [complement[base] for base in dna[::-1]] if type(dna) == list: return list_repr else: return &#39;&#39;.join(list_repr) . SPI1_best_motif = reverse_complement(SPI1_best_motif) . base_colors = {&#39;A&#39;: &#39;Lime&#39;, &#39;G&#39;: &#39;Gold&#39;, &#39;C&#39;: &#39;Blue&#39;, &#39;T&#39;:&#39;Crimson&#39;} . def print_bases(dna): return HTML(&#39;&#39;.join([f&#39;&lt;span style=&quot;color:{base_colors[base]};font-size:1.5rem;font-weight:bold;font-family:monospace&quot;&gt;{base}&lt;/span&gt;&#39; for base in dna])) . print_bases(SPI1_best_motif) . AAAAAGAGGAAGTA . SPI1/Pu.1 entry in HumanTFs database . def make_abs_pwm(pwm, ÎG_of_best_sequence): energy_range = abs(np.min(pwm, axis=1)) energy_fractional_allocation = energy_range/energy_range.sum() energy_per_base = ÎG_of_best_sequence * energy_fractional_allocation abs_pwm = pwm + np.expand_dims(energy_per_base, axis=1) return abs_pwm . &quot;microscale thermophoresis&quot; . (Pham, ... Rehli) . SPI1_best_nanomolar_kd = 156 SPI1_best_ÎG = kcal_ÎG_from_nanomolar_kd(SPI1_best_nanomolar_kd) abs_pwm = make_abs_pwm(pwm, SPI1_best_ÎG) . import matplotlib as mpl from matplotlib.text import TextPath from matplotlib.patches import PathPatch from matplotlib.font_manager import FontProperties import matplotlib.pyplot as plt fp = FontProperties(family=&quot;Arial&quot;, weight=&quot;bold&quot;) globscale = 1.35 LETTERS = { &quot;T&quot; : TextPath((-0.305, 0), &quot;T&quot;, size=1, prop=fp), &quot;G&quot; : TextPath((-0.384, 0), &quot;G&quot;, size=1, prop=fp), &quot;A&quot; : TextPath((-0.35, 0), &quot;A&quot;, size=1, prop=fp), &quot;C&quot; : TextPath((-0.366, 0), &quot;C&quot;, size=1, prop=fp) } def letterAt(letter, x, y, yscale=1, ax=None): text = LETTERS[letter] t = mpl.transforms.Affine2D().scale(1*globscale, yscale*globscale) + mpl.transforms.Affine2D().translate(x,y) + ax.transData p = PathPatch(text, lw=0, fc=base_colors[letter], transform=t, alpha=1-0.8*(y&lt;0)) if ax != None: ax.add_artist(p) return p def plot_abs_pwm(abs_pwm, ax=None): if ax is None: fig, ax = plt.subplots(figsize=(4,3)) x = 1 maxi = 0 for row in abs_pwm: y = 0 for dG, base in sorted(zip(row, base_index)): if dG &gt; 0: letterAt(base, x,y, dG, ax=ax) y += dG y = -0.1 for dG, base in sorted(zip(row, base_index), reverse=True): if dG &lt; 0: y += dG letterAt(base, x,y, abs(dG), ax=ax) x += 1 maxi = max(maxi, y) plt.xticks(range(1,x)) plt.xlim((0, x)) plt.ylim((-5, 2)) plt.tight_layout() plt.show() . def reverse_complement_pwm(pwm): return pwm[::-1, [base_index.index(complement[base]) for base in base_index]] . fig, ax = plt.subplots(figsize=(4,3)) plot_abs_pwm(reverse_complement_pwm(abs_pwm), ax=ax) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-13T16:12:15.634963 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ def get_hg38_bases(chrom, start, stop): url = f&#39;http://genome.ucsc.edu/cgi-bin/das/hg38/dna?segment={chrom}:{start},{stop}&#39; response = requests.get(url) root = ElementTree.fromstring(response.content) dna = root.find(&#39;SEQUENCE&#39;).find(&#39;DNA&#39;).text.replace(&#39; n&#39;, &#39;&#39;) return dna . chrom = &#39;chr11&#39; start = &#39;47378300&#39; stop = &#39;47378900&#39; dna = get_hg38_bases(chrom, start, stop) . dna . &#39;ggtactcacaggggggacgaggggaaacccttccattttgcacgcctgtaacatccagccgggctccgagtcggtcagatcccctgcctcggtgggggccaatgcagagcccctcaggatggggtgccccgtcaggggctggacggtcgtggggcgggtgcagggctcaggcctgccccctgagctacaggagccctgggtgagccccctcccttgacattgcagggccagcacaagttcctgattttatcgaagggcctgccgctgggagatagtccccttggggtgacatcaccgccccaacccgtttgcataaatctcttgcgctacatacaggaagtctctggccggctggggcaggtggtgctcaaagggctggcctgggaagccatggggtccaggccccctgcccagaggaagctgggactgagagggatgactttgggggctaagctggggagggaggatgggagggagaacgtgtagctctgccacaccactgggaggcttttgctctaacccaacaaatgcctgcttcttttgagatccctatgtagccaacagtcacctcattggggtcagagctggaaggggtggcctc&#39; . base_index = {&#39;a&#39;:0,&#39;c&#39;:1,&#39;g&#39;:2,&#39;t&#39;:3,&#39;A&#39;:0,&#39;C&#39;:1,&#39;G&#39;:2,&#39;T&#39;:3} binding_energies_at_starting_positions = [] for i in range(len(dna)-len(abs_pwm)): cumulative_energy_of_binding = 0 for j, row in enumerate(abs_pwm): cumulative_energy_of_binding += row[base_index[dna[i+j]]] binding_energies_at_starting_positions.append(cumulative_energy_of_binding) . binding_energies_at_starting_positions = pd.Series(np.where(binding_energies_at_starting_positions &gt; 0, binding_energies_at_starting_positions, np.nan)) . binding_energies_at_starting_positions.to_frame().reset_index().plot.scatter(x=&#39;index&#39;, y=0) . &lt;AxesSubplot:xlabel=&#39;index&#39;, ylabel=&#39;0&#39;&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-13T16:24:04.333801 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ # find other TFs likely to bind the SPI1 promoter, scan those, make plot like below. # make some code that turns something like the above into a bigWig and uploads it to ResGen # Scan key sequences with MOODS with my PWMs. And/or write my own PWM scanning code (copy from Nina class?) # find a credible absolute energy of binding for every TF: maybe by DBD family? # # What do I actually need to do now? # get a regulatory region. Scan all the probound motifs and newly converted energy motifs # get all the ChIP peaks that overlap the regulatory region # make a way to plot # - the ChIP datasets # - the ProBound binding energy tracks # - the old fashioned motif binding energy tracks # I need to be able to use my tool to make predictions about binding # for that I need nucleosome energies and MCMC up and running # Do I want a bed file or a bigwig? # # I have the HepG2 ATAC and RNA. I can plot those as well. . tfdb = pd.read_json(&#39;../../AChroMap/data/processed/TF.json&#39;, orient=&#39;index&#39;).dropna(subset=[&#39;pwm&#39;]) . SPI1_pwm = tfdb.loc[&#39;SPI1&#39;].pwm . def make_energy_pwm(pwm): pwm = np.array(pwm)+1e-5 pwm = pwm/np.expand_dims(np.sum(pwm, axis=1), axis=1) energy_pwm = np.array([[kbT*ln(fraction/max(row)) for fraction in row] for row in pwm]) return energy_pwm . make_energy_pwm(SPI1_pwm) . array([[ 0. , -1.20747651, -0.76329857, -1.30199406], [ 0. , -3.23336508, -1.58830463, -2.52713027], [ 0. , -4.80766653, -3.96580796, -3.15496939], [ 0. , -7.07564291, -7.07564291, -2.41959235], [ 0. , -4.36519752, -3.55755168, -1.37213046], [-2.8774099 , -1.88465617, 0. , -4.83042179], [-0.50434207, 0. , -1.06004198, -5.07709861], [-3.20513362, -5.24726386, 0. , -4.8363299 ], [-7.08749323, -5.25074443, 0. , -7.08749323], [ 0. , -4.81392442, -7.08724705, -5.22549716], [ 0. , -5.22742065, -5.22742065, -4.26112566], [-7.0662731 , -2.06233187, 0. , -5.24038105], [-3.36291984, -2.63161759, -3.70632155, 0. ], [ 0. , -1.33887465, -0.76383511, -0.34858339]]) . tfdb[&#39;energy_pwm&#39;] = tfdb.pwm.apply(make_energy_pwm) . Let&#39;s now proceed to a slightly more complex regulatory region, with 20 binding sites for 10 TFs . # draw our model regulatory region: TFs, TF_conc, TFBSs, cooperativities = create_environment(len_DNA=200, num_TFs=10, num_TFBS=20) ax = draw_config(TFBSs, TF_conc, cooperativities, len_DNA=200) plt.tight_layout() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-04T11:37:39.954187 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ Notice first that when binding sites overlap slightly, we&#39;ve introduced a free energy penalty of competition, denoted by a pink line between the binding sites, and when they stronly overlap, we&#39;ve disallowed those configurations, denoted by a red line. . We notice a problem with evaluating our $p_ mathrm{config}$ expression with realistic regulatory DNA: the number of configurations grows exponentially in the number of transcription factor binding sites ($2^{| mathrm{TFBS}|}$). Only 20 Transcription Factor Binding sites entails ~1 million configurations, and so ~1 million terms in the denominator $Z$. Computing probabilities of configurations exactly then becomes intractable for realistic scenarios: we need to proceed by sampling. . Thankfully there&#39;s a dynamic programming approach to computing Z. . config = np.round(np.random.rand(len(TFBSs))).astype(int) . Which we can plot, and compute the associated log-statistical-weight: . axs = draw_config(config) plt.tight_layout() print(&#39;log(p_config) =&#39;, log_P_config(config)) . log(p_config) = -1.4748275325663975 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2022-02-03T18:15:21.883707 image/svg+xml Matplotlib v3.5.1, https://matplotlib.org/ References . Calculating transcription factor binding maps for chromatin | . Multiple TFs: Nucleosome-mediated Cooperativity . Direct cooperativity (driven by protein-protein interactions) between transcription factors exerts significant control over transcription when it occurs, but it appears to be the exception, not the rule. More common is indirect cooperativity mediated by nucleosomes. Nucleosomes can be thought of as repressive transcription factors with large footprints and relatively non-specific sequence-dependent binding affinities. When multiple transcription factors bind independently in the footprint of a nucleosome, they do so cooperatively, despite not making physical contact with one another. . Including nucleosomes in our description of chromatin produces a new set of interactions, which require energy parameters. Those are: . Nucleosome:DNA interactions | Nucleosome:TF interactions | Nucleosome:Nucleosome interactions | . Nucleosome:DNA . Nucleosomes have at least a 5000-fold range of affinities for differing DNA sequences. . What controls nucleosome positions? Nucleosomes do not bind specific 147-bp sequences, but they do have preferences for certain periodic patterns, and disfavor certain other patterns. Nucleosomes are canonically wrapped by 147bp of DNA, but due to nucleosome crowding and nucleosome beathing, they are often wrapped by fewer bases. . For our model, we will say that any stretch of DNA [of lengths 100-147bp] can be wrapped by a nucleosome, with an energy which is derived from the [sequence model here]. . def nucleosome_energy_from_sequence(sequence): &#39;&#39;&#39; tells you the binding energy &#39;&#39;&#39; pass . . Nucleosome:TF . Nucleosomes and Transcription Factors were considered to sterically block one another from binding the same stretch of DNA, however, we now know that many TFs can co-bind DNA and stabilize nucleosomal binding to a piece of DNA. . For simplicity, we&#39;ll assume we&#39;re only considering transcription factors which cannot co-bind DNA with nucleosomes, and impose a large ÎG penalty for nucleosome and TF co-binding. . Nucleosome:Nucleosome . Nucleosomes are spaced by stretches of free DNA called linker DNA, which have characterisitic lengths. Those specific lengths are believed to facilitate nucleosome arrays forming higher-order chromatin structures. . def energy_of_spacing_between_nucleosomes(nucleosome_positions): &#39;&#39;&#39; &#39;&#39;&#39; pass . nanomolar_kd_from_kcal_ÎG(23.8*kbT) . 0.04610959744808222 . nanomolar_kd_from_kcal_ÎG(14.4*kbT) . 557.3903692694596 . send blog post for feedback: . Jeremy / Leonid | Advait / Bin Zhang | Anders | Kellis lab | Vlad Teif, other online nucleosome people | TF people? | Muir Morrison / Rob Philips | . transfer matrix: probably won&#39;t use . # the PWMs are concatenated head to tail on the y/i axis (and expanded ACGT). The next base is on the x/j axis: there are 4 values in the row. # the start of each motif is that value * concentration # I need hypothetical absolute max kd&#39;s ÎG&#39;s. Then everything is relative to that. Does the transfer matrix sum energies or something else? # how does the particular sequence enter the matrix multiplication? You&#39;re choosing a path through the matrix. # Does that mean the matrix at each base is different? Or there are 4 matrices again? # yeah I think there have to be 4 matrices (or 6 if you include methylation). Wait since matrices are for each pair of positions, we need 16 acctually? . sum(tfdb.log_pwm.apply(len)) . 12757 . transfer_matrix_a = np.zeros((len(tf_states), len(tf_states))) # transfer_matrix_c = np.zeros((len(tf_states), len(tf_states))) # transfer_matrix_g = np.zeros((len(tf_states), len(tf_states))) # transfer_matrix_t = np.zeros((len(tf_states), len(tf_states))) . tfdb.log_pwm . TFAP2A [[-0.5324, 0.34450000000000003, -1.5585, 0.530... TFAP2B [[-2.4892, 0.5962000000000001, 1.0115, -2.6127... TFAP2C [[-0.5975, -0.163, -1.1323, 0.7243], [-1.3232,... TFAP2D [[0.8455, -1.4861, 0.1063, -1.8089], [-5.6699,... TFAP2E [[-5.6699, 0.43660000000000004, 1.1706, -5.669... ... BATF3 [[-0.8147000000000001, 0.11900000000000001, 0.... CREB1 [[-0.049100000000000005, -0.20500000000000002,... TP53 [[0.1729, -0.6791, 0.6519, -0.661], [0.7944, -... TP63 [[-0.5391, -1.3228, 1.0218, -0.394300000000000... TP73 [[0.08610000000000001, -0.6413, 0.738200000000... Name: log_pwm, Length: 1080, dtype: object . base_index = {&#39;a&#39;:0,&#39;c&#39;:1,&#39;g&#39;:2,&#39;t&#39;:3} . tf_states = pd.Index([(tf, i) for tf, log_pwm in tfdb.log_pwm.items() for i in range(len(log_pwm))]) . def transfer_matrix_index(TF, offset): return tf_states.get_loc((TF, offset)) . for TF, pwm in tfdb.log_pwm.items(): for offset, row in enumerate(pwm): i = transfer_matrix_index(TF, offset) transfer_matrix_a[i][i+1] = pwm[offset][base_index[&#39;a&#39;]] . - IndexError Traceback (most recent call last) &lt;ipython-input-231-ffe1fe50396a&gt; in &lt;module&gt; 4 i = transfer_matrix_index(TF, offset) 5 -&gt; 6 transfer_matrix_a[i][i+1] = pwm[offset][base_index[&#39;a&#39;]] 7 IndexError: index 12757 is out of bounds for axis 0 with size 12757 .",
            "url": "https://alexlenail.me/back_of_my_envelope/2022/01/12/equilibrium_TFs.html",
            "relUrl": "/2022/01/12/equilibrium_TFs.html",
            "date": " â¢ Jan 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Enzyme Kinetic Parameter Inference",
            "content": "# imports import os, sys, pickle, time from itertools import combinations_with_replacement, product from collections import Counter, namedtuple from io import StringIO import h5py from pathlib import Path import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.ticker as mtick import matplotlib.patches as patches from matplotlib.colors import to_hex import plotly.graph_objs as go import plotly.express as px from plotly.subplots import make_subplots import seaborn as sns import scipy.stats from scipy.stats import multivariate_normal from ipywidgets import interact, interactive, IntSlider, SelectionSlider, fixed from IPython.display import display, clear_output %config InlineBackend.figure_format = &#39;retina&#39; %matplotlib inline # from IPython.display import set_matplotlib_formats # set_matplotlib_formats(&#39;svg&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 plt.rcParams[&#39;agg.path.chunksize&#39;] = 10000 plt.rcParams[&#39;animation.html&#39;] = &#39;jshtml&#39; plt.rcParams[&#39;hatch.linewidth&#39;] = 0.3 exp = np.exp sqrt = np.sqrt Î  = np.prod Ï = np.pi N = np.random.normal def hex_to_rgb(h): return [int(h.lstrip(&#39;#&#39;)[i:i+2], 16)/256 for i in (0, 2, 4)] matplotlib_colors = plt.rcParams[&#39;axes.prop_cycle&#39;].by_key()[&#39;color&#39;] import warnings warnings.filterwarnings(action=&#39;once&#39;) . # resize figure special function from IPython.core.display import Image, HTML import io import binascii def resize_fig(width, height): s = io.BytesIO() plt.savefig(s, format=&#39;png&#39;, bbox_inches=&quot;tight&quot;, dpi=200) plt.close() return f&#39;&lt;img width=&quot;{width}&quot; height=&quot;{height}&quot; class=&quot;keep_dims&quot; src=&quot;data:image/png;base64,{binascii.b2a_base64(s.getvalue()).decode()}&amp;#10;&quot;&gt;&#39; . Background . $$ newcommand{ kon}{k_{ mathrm{on}}} newcommand{ koff}{k_{ mathrm{off}}} newcommand{ kcat}{k_{ mathrm{cat}}} newcommand{ kuncat}{k_{ mathrm{uncat}}} newcommand{ kms}{k_{m, mathrm{S}}} newcommand{ kmp}{k_{m, mathrm{P}}} newcommand{ dSdt}{ frac{d[ mathrm{S}]}{dt}} newcommand{ dEdt}{ frac{d[ mathrm{E}]}{dt}} newcommand{ dESdt}{ frac{d[ mathrm{ES}]}{dt}} newcommand{ dPdt}{ frac{d[ mathrm{P}]}{dt}}$$Enzyme Kinetics . Enzymes catalyze many critical chemical reactions in cells. . Describing a cell with a mathematical model (a long-standing goal of computational biologists) would entail modelling each enzyme-catalyzed chemical reaction. . However, although we may know the scheme for many enzymatic reactions (the responsible enzyme, the associated substrates, and resultant products) we are often missing many of the details needed to construct a faithful mathematical model of the reaction. . Let&#39;s begin by introducing the mathematical model used to describe enzymatic reaction schemes. Consider the following enzymatically-catalyzed (uni uni) chemical reaction scheme: . $$ E+S underset{ koff}{ overset{ kon}{ rightleftarrows}} ES underset{ kuncat}{ overset{ kcat}{ rightleftarrows}}E+P $$ . In this scheme E is an enzyme, S is its substrate, ES is the enzyme-substrate complex, which is an intermediate, and P is the product of the reaction. Each of those chemical species has a concentration in a fixed volume, which we denote with brackets (e.g. $[ mathrm{E}]$ = enzyme concentration). . If we make the simplifying assumption that the 4 molecular species are &#39;well-mixed&#39; in solution, we can invoke the &#39;Law of Mass Action&#39; under which the rate of each of the four included reactions is linear in the concentrations of the reactants (with an associated coefficient called the rate constant). The reactions in the above scheme are: enzyme-substrate association ($ kon$), dissociation ($ koff$), enzyme catalysis of substrate into product ($ kcat$), and enzyme-product re-association (&quot;uncatalysis&quot;, $ kuncat$). The designation of &#39;substrate&#39; and &#39;product&#39; is our choice -- the model is entirely symmetric, which is reflected in the associated ODEs: . $$ begin{aligned} frac{d[ mathrm{S}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] frac{d[ mathrm{E}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{ES}]}{dt} &amp;= - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{P}]}{dt} &amp;= k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{aligned}$$This differential equation model describing the (deterministic) chemical kinetics for an enzymatically-catalyzed reaction in well-mixed conditions contains 4 kinetic parameters, i.e. 4 degrees of freedom, which we do not know a priori. These will be the subject of inference. . Note: the intracellular environment is not best described as well-mixed, and models of &#8217;Macromolecular Crowding&#8217; have led to more accurate rate laws for these reactions in vivo. However, we will retain the well-mixed assumption for now. . Parameter Inference . There are 3 typical problems associated with ODE models: . Supplied with a complete specification of the system, the forward problem is to integrate the differential equations from some initial conditions forwards in time and predict the trajectory of the system. This is what is typically meant by &quot;solving&quot; the ODE system, but exact analytical solutions are rare, and numerical methods are often brought to bear to approximate system trajectories. | Supplied with one or more trajectories (data) but incomplete specification of the system, the inverse problem is to estimate parameters of the system (coefficients in the ODE expressions). | Finally, given some manipulable inputs, the control problem is to drive the system towards some desired state. | . This post will explore a range of approaches for the inverse problem. Our goal will be to estimate the kinetic parameters of enzymatically-catalyzed chemical reactions from timeseries of concentrations of the molecular species. . Note: enzyme kinetic parameters are typically not inferred from metabolite timeseries data using the methods we will describe, but instead from specific enzyme assays. However, at the moment, these assays are limited to studying one enzyme at a time. The inference approaches described in this post can leverage data from emerging high-throughput assays, which measure many molecular concentrations at once. . The determination of the kinetic parameters for the enzymatic reactions of life is a major project, and reported values have been tabulated in databases such as BRENDA. However, my experience with these databases has been that the reported kinetic parameters are not internally consistent. . The Michaelis-Menten/Briggs-Haldane Approximation . Two assumptions commonly made at this point are: . to assume the initial substrate concentration is much larger than the enzyme concentration ($[ mathrm{S_0}] gg [ mathrm{E_0}]$). | to suppose that the rates of enzyme-substrate association ($ kon$) and dissociation ($ koff$) are greater than the rates of catalysis and uncatalysis (i.e. $ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$). | These assumptions permit a timescale separation argument called the &quot;Quasi-Steady-State Approximation&quot; (QSSA) in which we set $ dESdt = 0$, which enables the derivation of the traditional Reversible Michaelis-Menten/Briggs-Haldane expression: . $$ begin{aligned} frac{d[ mathrm{P}]}{dt} &amp;= frac{ frac{ kcat , [ mathrm{E_T}] [ mathrm{S}]}{K_{m, mathrm{S}}} - frac{ koff , [ mathrm{E_T}] [ mathrm{P}]}{K_{m, mathrm{P}}}} {1+ frac{[ mathrm{S}]}{K_{m, mathrm{S}}} + frac{[ mathrm{P}]}{K_{m, mathrm{P}}}} frac{d[ mathrm{S}]}{dt} &amp;= - frac{d[ mathrm{P}]}{dt} end{aligned}$$in which we have introduced the &quot;Michaelis Constants&quot;: $K_{m, mathrm{S}} = frac{ koff + kcat}{ kon}$ and $K_{m, mathrm{P}} = frac{ koff + kcat}{ kuncat}$. . The QSSA reduces the system from 4 variables to 2. However, there are still 4 kinetic parameters to estimate in this reduced model. . Note: another assumption typically made at this point is to assume that catalysis is irreversible ($ kuncat = 0$), leading to a further simplified expression for the rate of product formation $ frac{d[ mathrm{P}]}{dt}$. However, this assumption is quite often inaccurate, so we will not make it. . Exploring the Forward Model . A Standard Example . Before we explore techniques to estimate enzyme kinetic parameters from timeseries data, we need to generate timeseries data to begin with. We can accomplish that by fixing kinetic parameters, then solving the forward problem. It will turn out that integrating the differential equations forwards is a subroutine of both approaches to the inverse problem we&#39;ll see in this post, so developing a method for the forward problem is hardly wasted effort. . In order to produce a trajectory, we need to set initial conditions. We&#39;ll integrate the reaction kinetics of a hypothetical in vitro experiment, in which a fixed quantity of enzyme and substrate are added to the reaction at the outset, then left to react. . Note: in vivo we would expect the concetration of enzyme to vary over time, and the substrate to be replenished. We will generalize this approach to a more biologically-relevant setting in a future post. . Our initial conditions are: . $[E]_0$, the initial enzyme concentration, is set to 1mM (miliMolar, i.e. 1000Î¼M). | $[S]_0$, the initial substrate concentration is set to 10mM. | . default_initial_conditions = { &#39;S_0&#39;: 10e3, &#39;E_0&#39;: 1e3, &#39;ES_0&#39;: 0.0, &#39;P_0&#39;: 0.0 } . Next, let&#39;s fix some generic rate constants: . $ kon ,$ of $10^6$ events per Mol per second, or 1 per Î¼M per second, is a typical rate for enzyme-substrate binding. | $ koff ,$ of 500/s results in a $ koff$/$ kon$ = $k_d$ of 500 Î¼M, which is a typical $k_d$. | $ kcat ,$ is 30/s, a fairly slow but respectable $ kcat$. | $ kuncat ,$ of $ frac{ kon}{10}$ is often considered as the boundary for the QSSA to hold (so 0.1 per Î¼M per second). Let&#39;s use $ kuncat = frac{ kon}{100} = $ 0.01/Î¼M for good measure. | . Our units are Î¼M and seconds. . default_kinetic_params = { &#39;k_on&#39;: 1, &#39;k_off&#39;: 500, &#39;k_cat&#39;: 30, &#39;k_uncat&#39;: 0.01 } def k_ms(p): return (p[&#39;k_off&#39;] + p[&#39;k_cat&#39;]) / p[&#39;k_on&#39;] def k_mp(p): return (p[&#39;k_off&#39;] + p[&#39;k_cat&#39;]) / p[&#39;k_uncat&#39;] default_kinetic_params[&#39;k_ms&#39;] = k_ms(default_kinetic_params) default_kinetic_params[&#39;k_mp&#39;] = k_mp(default_kinetic_params) . To simulate the kinetics with little derivative steps, we need a step size, and a number of total steps: . dt = 1e-6 steps = 5e5 . There are a variety of numerical methods to integrate systems of differential equations. The most straightforward is Euler&#39;s method, which we&#39;ve written down explicitly for this system below: . def integrate_euler_full(kinetic_params, dt=dt, steps=steps, initial_conditions=default_initial_conditions): S, E, ES, P = initial_conditions.values() k_on, k_off, k_cat, k_uncat, k_ms, k_mp = kinetic_params.values() traj = [[S, E, ES, P]] for _ in range(int(steps)): dS = k_off * ES - k_on * E * S dE = k_off * ES - k_on * E * S + k_cat * ES - k_uncat * E * P dES = k_on * E * S - k_off * ES - k_cat * ES + k_uncat * E * P dP = k_cat * ES - k_uncat * E * P S += dS * dt E += dE * dt ES += dES * dt P += dP * dt traj.append([S, E, ES, P]) return pd.DataFrame(traj, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;], index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)) . We&#39;ll also write down Euler&#39;s method for the Michaelis-Menten/Briggs-Haldane kinetics . # define integrate_euler_MM(), which integrates the Michaelis-Menten/Briggs-Haldane kinetics def integrate_euler_MM(kinetic_params, dt=dt, steps=steps, initial_conditions=default_initial_conditions): S, E, ES, P = initial_conditions.values() k_on, k_off, k_cat, k_uncat, k_ms, k_mp = kinetic_params.values() traj = [P] for _ in range(int(steps)): dP = ((k_cat * E * S) / k_ms - (k_off * E * P) / k_mp) / (1 + S / k_ms + P / k_mp) dS = -dP P += dP * dt S += dS * dt traj.append(P) return pd.Series(traj, name=&#39;P_MM&#39;, index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)).to_frame() . Now we can integrate the reaction kinetics, and plot the trajectory. We&#39;ll overlay the Michaelis-Menten/Briggs-Haldane kinetics with dotted lines on top of the full kinetics (solid). . traj_euler_full = integrate_euler_full(default_kinetic_params) traj_euler_mm = integrate_euler_MM(default_kinetic_params) . # figure styles def fig_style(ax): for side in [&quot;right&quot;,&quot;top&quot;]: ax.spines[side].set_visible(False) ax.set_xlabel(&#39;time (s)&#39;, weight=&#39;bold&#39;) ax.set_ylabel(&#39;concentration (Î¼M)&#39;, weight=&#39;bold&#39;) def param_string(E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None, **kwargs): return f&#39;[k_on= {k_on}/Î¼M/s] [k_off = {k_off}/s] [k_cat = {k_cat}/s] [k_uncat = {k_uncat}/Î¼M/s] [Eâ = {int(E_0)}Î¼M] [Sâ = {int(S_0)}Î¼M]&#39; c = { &#39;S&#39;: &#39;dodgerblue&#39;, &#39;E&#39;: &#39;sienna&#39;, &#39;ES&#39;: &#39;blue&#39;, &#39;P&#39;: &#39;darkblue&#39;, &#39;S_MM&#39;: &#39;steelblue&#39;, &#39;P_MM&#39;: &#39;slateblue&#39;, &#39;k_on&#39;: &#39;mediumseagreen&#39;, &#39;k_off&#39;: &#39;olive&#39;, &#39;k_cat&#39;: &#39;darkgreen&#39;, &#39;k_uncat&#39;: &#39;darkgoldenrod&#39;, &#39;k_m&#39;: &#39;olivedrab&#39;, &#39;k_ms&#39;: &#39;forestgreen&#39;, &#39;k_mp&#39;: &#39;darkkhaki&#39;, } c = {k:to_hex(v) for k,v in c.items()} def color(columns): return [c[col] for col in columns] . # plot the integrated kinetics ax = traj_euler_full.plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(traj_euler_full.columns)) traj_euler_mm.plot.line(ax=ax, color=color(traj_euler_mm.columns), linestyle=&#39;--&#39;) fig_style(ax) . We can plainly see the validity of the Quasi-Steady-State Approximation (QSSA) in action in the trajectory: Enzyme E and Substrate S rapidly form Enzyme-Substrate complex ES, the concentration of which remains relatively constant throughout the course of the reaction (recall the QSSA is the approximation that $ dESdt = 0$). Thus, the Michaelis-Menten/Briggs-Haldane product concentration trajectory P_MM well approximates the full kinetics trajectory for the concentration of product P, since the requisite assumptions are valid, namely, (1) $[ mathrm{S_0}] gg [ mathrm{E_0}]$ and (2) $ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$. . In practice, Michaelis-Menten/Briggs-Haldane kinetics are often assumed by default, risking the possibility of their misapplication. Let&#39;s take this opportunity to explore how the MM/BH kinetics diverge from the full kinetics when we violate the requisite assumptions. . Breaking the Michaelis-Menten/Briggs-Haldane Assumptions: Initial Substrate:Enzyme Ratio . Suppose first the number of molecules of substrate is not much greater than the number of molecules of enzyme, which is a plausible regime for certain reactions in vivo. . initial_conditions = { &#39;S_0&#39;: 2e3, &#39;E_0&#39;: 1e3, &#39;ES_0&#39;: 0.0, &#39;P_0&#39;: 0.0 } . traj_euler_full_2 = integrate_euler_full(default_kinetic_params, steps=2e5, initial_conditions=initial_conditions) traj_euler_mm_2 = integrate_euler_MM(default_kinetic_params, steps=2e5, initial_conditions=initial_conditions) ax = traj_euler_full_2.plot.line(title=param_string(**initial_conditions, **default_kinetic_params), color=color(traj_euler_full_2.columns)) traj_euler_mm_2.plot.line(ax=ax, color=color(traj_euler_mm_2.columns), linestyle=&#39;--&#39;) fig_style(ax) . Then P_MM worsens significantly as an estimate of P. . Breaking the Michaelis-Menten/Briggs-Haldane Assumptions: Fast Enzyme-Substrate Complex Kinetics . Suppose further that the rates of association and dissociation of enzyme with subtstrate are not substantially faster than those of enzyme and product. . kinetic_params = { &#39;k_on&#39;: 0.05, &#39;k_off&#39;: 1, &#39;k_cat&#39;: 50, &#39;k_uncat&#39;: 0.5 } kinetic_params[&#39;k_ms&#39;] = k_ms(kinetic_params) kinetic_params[&#39;k_mp&#39;] = k_mp(kinetic_params) . traj_euler_full_3 = integrate_euler_full(kinetic_params, initial_conditions=initial_conditions) traj_euler_mm_3 = integrate_euler_MM(kinetic_params, initial_conditions=initial_conditions) ax = traj_euler_full_3.plot.line(title=param_string(**initial_conditions, **kinetic_params), color=color(traj_euler_full_3.columns)) traj_euler_mm_3.plot.line(ax=ax, color=color(traj_euler_mm_3.columns), linestyle=&#39;--&#39;) fig_style(ax) . Then the Michaelis-Menten/Briggs-Haldane kinetics diverge further. . In each of these latter trajectories, the criteria to make the Michaelis-Menten/Briggs-Haldane approximation are violated, leading to poor approximations to the full kinetics. We belabor this point here because in the following, we will seek to infer the parameters of the kinetics, and our inference will fit poorly if we fit to inappropriate kinetic expressions. . Comparing Integrators . All of the above trajectories are generated by Euler&#39;s Method, the most intuitive ODE integration technique. Unfortunately, Euler&#39;s Method&#39;s naÃ¯vete has drawbacks: . The order of the error is large with respect to the timestep size. | The method is slow, due to the uniformity of the timestep sizes. | . A variety of faster and more accurate (albeit more complicated) integrators have been proposed, many of which have implementations in scipy&#39;s integrate package. Due to their superior speeds and accuracies, we&#39;ll use these methods during inference. As a sanity check, we compare our basic Euler Method solver to scipy&#39;s: . # define integrate_scipy_full and scipy_MM integrate_scipy_MM (and helpers) to integrate chemical kinetics with scipy from scipy.integrate import solve_ivp def dy_full(t, y, k_on, k_off, k_cat, k_uncat, *args): # Y ordered S,E,ES,P dy = np.zeros(4) dy[0] = k_off * y[2] - k_on * y[1] * y[0] dy[1] = k_off * y[2] - k_on * y[1] * y[0] + k_cat * y[2] - k_uncat * y[1] * y[3] dy[2] = k_on * y[1] * y[0] - k_off * y[2] - k_cat * y[2] + k_uncat * y[1] * y[3] dy[3] = k_cat * y[2] - k_uncat * y[1] * y[3] return dy def dy_MM(t, y, S_0, E_0, ES_0, P_0, k_on, k_off, k_cat, k_uncat, k_ms, k_mp): # Y ordered S,P dy = np.zeros(2) dy[1] = ((k_cat * E_0 * y[0]) / k_ms - (k_off * E_0 * y[1]) / k_mp) / (1 + y[0] / k_ms + y[1] / k_mp) dy[0] = -dy[1] return dy def integrate_scipy_full(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps, atol=1e-6): t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = list(initial_conditions.values()) try: sol = solve_ivp(dy_full, t_span, y0, args=(*kinetic_params.values(),), t_eval=t_eval, first_step=dt, method=&#39;LSODA&#39;, atol=atol) return pd.DataFrame(sol.y.T, index=sol.t, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) except: return pd.DataFrame(columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) def integrate_scipy_MM(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps): t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = [initial_conditions[&#39;S_0&#39;], initial_conditions[&#39;P_0&#39;]] try: sol = solve_ivp(dy_MM, t_span, y0, args=(*initial_conditions.values(), *kinetic_params.values()), t_eval=t_eval, first_step=dt) return pd.DataFrame(sol.y.T, index=sol.t, columns=[&#39;S_MM&#39;, &#39;P_MM&#39;]) except: return pd.DataFrame(columns=[&#39;S_MM&#39;, &#39;P_MM&#39;]) . # solve the system with both integrators, for default parameters traj_scipy_full = integrate_scipy_full(default_kinetic_params) ax = traj_scipy_full.plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(traj_scipy_full.columns), alpha=0.5) traj_euler_full.plot.line(ax=ax, color=color(traj_euler_full.columns), linestyle=&#39;--&#39;) fig_style(ax) . # solve the system with both integrators, for unusual parameters kinetic_params = { &#39;k_on&#39;: 0.02, &#39;k_off&#39;: 5, &#39;k_cat&#39;: 10, &#39;k_uncat&#39;: 0.00001, } kinetic_params[&#39;k_ms&#39;] = k_ms(kinetic_params) kinetic_params[&#39;k_mp&#39;] = k_mp(kinetic_params) start = time.process_time() traj_euler_full_4 = integrate_euler_full(kinetic_params, initial_conditions=initial_conditions) euler_time = time.process_time() - start start = time.process_time() traj_scipy_full_4 = integrate_scipy_full(kinetic_params, initial_conditions=initial_conditions) scipy_time = time.process_time() - start ax = traj_scipy_full_4.plot.line(title=param_string(**initial_conditions, **kinetic_params), color=color(traj_scipy_full_4.columns), alpha=0.5) traj_euler_full_4.plot.line(ax=ax, color=color(traj_euler_full_4.columns), linestyle=&#39;--&#39;) fig_style(ax) . The lack of deviation gives us confidence both integration techniques are accurate. Meanwhile, . f&#39;our naÃ¯ve code takes {round(euler_time, 2)}s, whereas the optimized scipy code takes {round(scipy_time, 4)}s to generate the same trajectory.&#39; . &#39;our naÃ¯ve code takes 1.16s, whereas the optimized scipy code takes 0.0064s to generate the same trajectory.&#39; . Inference . We have seen how the trajectory of the chemical system is a function of the kinetic parameters. We would now like to invert that function to recover the kinetic parameters from an observed trajectory. . Suppose we know the initial concentrations of Enzyme E and Substrate S, and we measure the concentration of product P over the course of the reaction, which yields the following dataset: . # plot inverse problem setting measurement_times = np.arange(10+1)/20 observations = traj_scipy_full.loc[measurement_times, &#39;P&#39;] ax = observations.plot.line(marker=&#39;o&#39;, lw=0, color=color([&#39;P&#39;]), legend=True) traj_scipy_full.loc[0, [&#39;E&#39;, &#39;S&#39;]].to_frame().T.plot.line(ax=ax, marker=&#39;o&#39;, lw=0, color=color(traj_scipy_full.columns), legend=True) fig_style(ax) . There are two families of approaches to solving this inverse problem. We will explore the simplest variant of each type. . Note: If we had measurements for $ dPdt$ for various concentrations of $[ mathrm{S}]$, $[ mathrm{P}]$, and $[ mathrm{E}]$ we could estimate the kinetic parameters via nonlinear regression, a much simpler approach, which some readers may be familiar with. Concretely, supposing we had a set of measurements for the variables in brown, a nonlinear regression would permit us to fit the parameters in blue: $$ color{saddlebrown}{ dPdt} color{black} = frac{ frac{ color{dodgerblue}{ kcat} , color{saddlebrown}{[ mathrm{E_T}]} color{saddlebrown}{[ mathrm{S}]}} { color{dodgerblue}{K_{m, mathrm{S}}}} - frac{ color{dodgerblue}{ koff} , color{saddlebrown}{[ mathrm{E_T}]} color{saddlebrown}{[ mathrm{P}]}}{ color{dodgerblue}{K_{m, mathrm{P}}}}} {1+ frac{ color{saddlebrown}{[ mathrm{S}]}}{ color{dodgerblue}{K_{m, mathrm{S}}}} + frac{ color{saddlebrown}{[ mathrm{P}]}}{ color{dodgerblue}{K_{m, mathrm{P}}}}} $$ If we had assumed the reaction were irreversible ($k_{ mathrm{uncat}} = 0$), the Michaelis-Menten/Briggs-Haldane expression would have simplified further to $$ color{saddlebrown}{ dPdt} color{black} = frac{ color{dodgerblue}{ kcat} , color{saddlebrown}{[ mathrm{E_T}]} color{saddlebrown}{[ mathrm{S}]}} { color{dodgerblue}{K_{m, mathrm{S}}} color{black} + color{saddlebrown}{[ mathrm{S}]}} $$ Where $ color{dodgerblue}{ kcat} , color{saddlebrown}{[ mathrm{E_T}]}$ is often consolidated as $ color{dodgerblue}{V_{max}}$. It&#39;s difficult to imagine an assay for the activity of many enzymes in a cell at once. Careful Mass Spectrometry-based metabolomics and proteomics, or clever microscopy might measure $[ mathrm{S}]$, $[ mathrm{P}]$, and $[ mathrm{E}]$ but unlikely $ dPdt$ for many enzymes. We would presumably not be able to approximate $ dPdt$ via finite differences either, due to the likely sparsity of the measurement in time compared to the rates of the reactions. The computational approaches covered in this post are more elaborate (and expensive), but do not require measurement of $ dPdt$, making them better suited to existing assays. Bayesian Approach: Inference by Sampling . [We assume the reader is familiar with Bayesian Inference in other settings.] . The goal of the Bayesian approach is to characterize a posterior distribution of kinetic parameters which could plausibly have generated the data. Bayes Theorem defines the posterior as the product of the prior and likelihood (up to a constant factor). Thus the Bayesian approach entails defining a prior and a likelihood for our problem. . Prior . If the kinetic parameters of our enzyme are not unlike the kinetic parameters of other enzymes, then the empirical distribution of kinetic parameters of other enzymes is a good prior for the parameters of our enzyme. . Since databases of observed enzyme kinetic parameters (e.g. BRENDA, SabioRK) appear to be unreliable, we&#39;ll use a previously curated set of kinetic parameters (from the supplement of Bar-Even et. al.). . This database lists $k_{ mathrm{m}}$ and $ kcat$ for both &quot;forwards&quot; and &quot;reverse&quot; reactions with respect to which direction biologists believe is &quot;productive&quot;, from which we can parlay distributions for $ kms$ and $ kcat$ from reactions in the forwards direction, and $ kmp$ and $ koff$ from reverse reactions. . # import kinetic parameter database df = pd.read_excel(&#39;../data/Enzyme_Kinetic_Parameter_Inference/Moderately_Efficient_Enzyme/bi2002289_si_003.xls&#39;, 1)[[&#39;Reaction direction (KEGG)&#39;,&#39;KM (ÂµM)&#39;,&#39;kcat (1/sec)&#39;]] empirical_kms = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, &#39;KM (ÂµM)&#39;].dropna().rename(&#39;k_ms&#39;) empirical_kmp = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, &#39;KM (ÂµM)&#39;].dropna().rename(&#39;k_mp&#39;) empirical_kcat = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, &#39;kcat (1/sec)&#39;].dropna().rename(&#39;k_cat&#39;) empirical_koff = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, &#39;kcat (1/sec)&#39;].dropna().rename(&#39;k_off&#39;) empirical_joint_forward_params = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, [&#39;KM (ÂµM)&#39;,&#39;kcat (1/sec)&#39;]].dropna().rename(columns={&#39;KM (ÂµM)&#39;:&#39;k_ms&#39;, &#39;kcat (1/sec)&#39;:&#39;k_cat&#39;}) empirical_joint_reverse_params = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, [&#39;KM (ÂµM)&#39;,&#39;kcat (1/sec)&#39;]].dropna().rename(columns={&#39;KM (ÂµM)&#39;:&#39;k_mp&#39;, &#39;kcat (1/sec)&#39;:&#39;k_off&#39;}) . # figure styles def fig_style_2(ax): for side in [&quot;right&quot;,&quot;top&quot;,&quot;left&quot;]: ax.spines[side].set_visible(False) ax.get_yaxis().set_visible(False) . # plot km distribution in log-space log_empirical_kms = np.log(empirical_kms) log_empirical_kmp = np.log(empirical_kmp) log_kms_normal = scipy.stats.norm(loc=log_empirical_kms.mean(), scale=log_empirical_kms.std()) log_kmp_normal = scipy.stats.norm(loc=log_empirical_kmp.mean(), scale=log_empirical_kmp.std()) ax = log_empirical_kms.plot.hist(bins=500, alpha=0.3, density=1, legend=True) log_empirical_kmp.plot.hist(bins=500, ax=ax, alpha=0.3, density=1, legend=True) ax.set_xlabel(&#39;log(k_m[ÂµM]) histogram&#39;, weight=&#39;bold&#39;) fig_style_2(ax) # x1 = np.linspace(log_kms_normal.ppf(0.01), log_kms_normal.ppf(0.99), 100) # ax.plot(x1, log_kms_normal.pdf(x1)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.6, color=&#39;dodgerblue&#39;) # x2 = np.linspace(log_kmp_normal.ppf(0.01), log_kmp_normal.ppf(0.99), 100) # ax.plot(x2, log_kmp_normal.pdf(x2)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.8, color=&#39;peru&#39;) . This plot is surprising: according to this database, enzymes appear to have roughly equal binding affinity for their substrates and products. . # plot kcat distribution in log-space log_empirical_kcat = np.log(empirical_kcat) log_empirical_koff = np.log(empirical_koff) log_kcat_normal = scipy.stats.norm(loc=log_empirical_kcat.mean(), scale=log_empirical_kcat.std()) log_koff_normal = scipy.stats.norm(loc=log_empirical_koff.mean(), scale=log_empirical_koff.std()) ax = log_empirical_kcat.plot.hist(bins=500, alpha=0.3, density=1, legend=True) log_empirical_koff.plot.hist(bins=500, ax=ax, alpha=0.3, density=1, legend=True) x1 = np.linspace(log_kcat_normal.ppf(0.01), log_kcat_normal.ppf(0.99), 100) ax.plot(x1, log_kcat_normal.pdf(x1)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.4, color=&#39;dodgerblue&#39;) x2 = np.linspace(log_koff_normal.ppf(0.01), log_koff_normal.ppf(0.99), 100) ax.plot(x2, log_koff_normal.pdf(x2)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.6, color=&#39;peru&#39;) ax.set_xlabel(&#39;log(k_cat[1/s]) histogram&#39;, weight=&#39;bold&#39;) fig_style_2(ax) . On the other hand, they have a fairly strong preference for catalyzing the reaction biologists think of as forwards (~10x). . Since these empirical distributions over $ kms$ and $ kcat$ in the forwards direction and $ kmp$ and $ koff$ in the reverse direction look sufficiently like normals in log space, so we&#39;ll treat them as lognormals. However, we would like our inference procedure to estimate $ kon$, $ koff$, $ kcat$, and $ kuncat$. We can rearrange the expressions for $ kms$ and $ kmp$ to get expressions for the two parameters we&#39;re missing: . $$ kon = frac{ koff + kcat}{ kms} quad mathrm{and} quad kuncat = frac{ koff + kcat}{ kmp}$$ . Conveniently, the ratio of lognormal variables $ frac{X_1}{X_2}$ is also lognormal with $ mu_{1/2} = mu_1 - mu_2$ and $ sigma^2_{1/2} = sigma^2_1 + sigma^2_2 - sigma_{x_1, x_2}$. In order to use that fact, we say the sum of the random variables $ koff + kcat$ is also log-normally distributed. We compute its mean and variance empirically. . kcat_plus_koff = pd.Series(np.repeat(empirical_kcat.values, len(empirical_koff)) + np.tile(empirical_koff.values, len(empirical_kcat))) log_kcat_plus_koff_mean = np.log(kcat_plus_koff).mean() log_kcat_plus_koff_var = np.log(kcat_plus_koff).var() . This permits us to produce empirical distributions for $ kon$ and $ kuncat$, . log_kon_normal = scipy.stats.norm(loc=log_kcat_plus_koff_mean-log_empirical_kms.mean(), scale=sqrt(log_kcat_plus_koff_var+log_empirical_kms.var())) log_kuncat_normal = scipy.stats.norm(loc=log_kcat_plus_koff_mean-log_empirical_kmp.mean(), scale=sqrt(log_kcat_plus_koff_var+log_empirical_kmp.var())) . which, along with our empirical distributions for $ koff$ and $ kcat$, define a prior over the 4 kinetic parameters we wish to infer. . We might ask whether these are correlated lognormals... . pp = sns.pairplot(np.log(empirical_joint_forward_params), kind=&quot;kde&quot;, plot_kws={&#39;linewidths&#39;:0.5, &#39;color&#39;:&#39;darkolivegreen&#39;}) k_ms_univariate_density = pp.diag_axes[0].get_children()[0] k_ms_univariate_density.set_edgecolor(c[&#39;k_ms&#39;]) k_ms_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_ms&#39;]) + [0.1]) k_cat_univariate_density = pp.diag_axes[1].get_children()[0] k_cat_univariate_density.set_edgecolor(c[&#39;k_cat&#39;]) k_cat_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_cat&#39;]) + [0.1]) fig1 = resize_fig(400, 400) pp = sns.pairplot(np.log(empirical_joint_reverse_params), kind=&quot;kde&quot;, plot_kws={&#39;linewidths&#39;:0.5, &#39;color&#39;:&#39;grey&#39;}) k_mp_univariate_density = pp.diag_axes[0].get_children()[0] k_mp_univariate_density.set_edgecolor(c[&#39;k_mp&#39;]) k_mp_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_mp&#39;]) + [0.1]) k_off_univariate_density = pp.diag_axes[1].get_children()[0] k_off_univariate_density.set_edgecolor(c[&#39;k_off&#39;]) k_off_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_off&#39;]) + [0.1]) fig2 = resize_fig(400, 400) HTML(&#39;&lt;style&gt;.keep_dims{float:left;}&lt;/style&gt;&#39;+fig1+fig2+&#39;&lt;div style=&quot;clear: both;&quot;&gt;&lt;/div&gt;&#39;) . ...Not enough to include covariances in the prior. We set the prior covariance to be a diagonal matrix: . # define prior_cov to be a diagonal covariance matrix prior_cov = np.diag([log_kon_normal.var(), log_koff_normal.var(), log_kcat_normal.var(), log_kuncat_normal.var()]) . # define functions relating to the prior distribution def prior_pdf(k_on=None, k_off=None, k_cat=None, k_uncat=None, **kwargs): return ( log_kon_normal.pdf(k_on) * log_koff_normal.pdf(k_off) * log_kcat_normal.pdf(k_cat) * log_kuncat_normal.pdf(k_uncat)) def prior_logpdf(k_on=None, k_off=None, k_cat=None, k_uncat=None, **kwargs): return ( log_kon_normal.logpdf(k_on) + log_koff_normal.logpdf(k_off) + log_kcat_normal.logpdf(k_cat) + log_kuncat_normal.logpdf(k_uncat)) def sample_prior(): # returns [k_on, k_off, k_cat, k_uncat] return { &#39;k_on&#39;: log_kon_normal.rvs(), &#39;k_off&#39;: log_koff_normal.rvs(), &#39;k_cat&#39;: log_kcat_normal.rvs(), &#39;k_uncat&#39;: log_kuncat_normal.rvs()} . Now that we have a prior, let&#39;s examine where the default parameters introduced in Â§2.1 land in this distribution. I had claimed they were &quot;typical&quot;. . # plot log-space kinetic parameter distributions, with default parameters presented previously overlaid fig, axs = plt.subplots(2,2,constrained_layout=True) def plot_distrib(distrib, ax, title, param): ax.set_xlim(-14,14) ax.set_ylim(0,0.15) x = np.linspace(distrib.ppf(0.001), distrib.ppf(0.999), 100) y = distrib.pdf(x) color = c[param] ax.plot(x, y, &#39;-&#39;, lw=0.7, color=color) ax.fill_between(x, 0, y, color=color, alpha=0.1) ax.axvline(np.log(default_kinetic_params[param]), 0, 1, linestyle=&#39;--&#39;, color=color) ax.xaxis.set_ticks(np.arange(-14, 14.01, 2)) ax.set_xlabel(title, weight=&#39;bold&#39;) fig_style_2(ax) plot_distrib(log_kon_normal, axs[0][0], &#39;log(k_on[ÂµM])&#39;, &#39;k_on&#39;) plot_distrib(log_koff_normal, axs[0][1], &#39;log(k_off[1/s])&#39;, &#39;k_off&#39;) plot_distrib(log_kuncat_normal, axs[1][0], &#39;log(k_uncat[ÂµM])&#39;, &#39;k_uncat&#39;) plot_distrib(log_kcat_normal, axs[1][1], &#39;log(k_cat[1/s])&#39;, &#39;k_cat&#39;) . Likelihood . We need to define a likelihood $p(D| theta)$ which measures the probability of producing the observed data given settings of the kinetic parameters $ theta = { kon, koff, kcat, kuncat }$. Our data $D = { mathrm{obs}_{[ mathrm{P}]}(t) , ; t in 0...0.5 }$ are an observed trajectory of concentrations of reaction product P. Each setting of the kinetic parameters corresponds to a trajectory of concentrations of P (via a numerical integration). Intuitively, parameter sets which result in trajectories very near the observed trajectory are more likely. Therefore, our likelihood should measure the distance between the observed $ { mathrm{obs}_{[ mathrm{P}]}(t) , ; t in 0...0.5 }$ and predicted $ { u_{[ mathrm{P}]}(t, theta) , ; t in 0...0.5 }$. . How far should the predicted trajectory be allowed to stray from the measured $ { mathrm{obs}_{[ mathrm{P}]}(t) }$? The likelihood is really our statement about the presumed noise in our measurements. If we believe our measurements to be noiseless, then our likelihood should concentrate tightly around our measurements (a dirac $ delta$ in the limit), and we would only admit kinetic parameters that interpolate the observed $ { mathrm{obs}_{[ mathrm{P}]}(t) }$ almost exactly. In reality, no measurement is noiseless, so we propose the following noise model: . Supposing the detection of each molecule of P is an independent binary random variable with error rate $ sigma$ then the aggregate random variable $ mathrm{N}_{[ mathrm{P}]}(t)$ is gaussian-distributed $ sim mathcal{N}( mathrm{obs}_{[ mathrm{P}]}(t), , sigma cdot sqrt{ mathrm{obs}_{[ mathrm{P}]}(t)} )$. The variance of the gaussian grows as the square root of the mean, via a Central Limit Theorem argument. We can represent this noise model (and consequently, likelihood) visually as: . Ï = 5 # arbitrary magic number represents detection noise level . # plot intuition for likelihood definition plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] ax = observations.rename(&#39;observations&#39;).plot.line(marker=&#39;.&#39;, lw=0, color=color([&#39;P&#39;]), legend=True, markersize=2) domain = np.linspace(0,10000,1000) for x, y in observations.iloc[1:].items(): distrib = scipy.stats.norm(loc=y, scale=np.sqrt(y)*Ï) ax.fill_betweenx(domain, [x]*1000, x2=x+distrib.pdf(domain)/distrib.pdf(y)/150, color=color([&#39;P&#39;]), alpha=0.2) fig_style(ax) . Concretely, the likelihood is the product distribution of each of the gaussian marginals centered around the measurements. These form a multivariate normal, diagonal since we don&#39;t model covariances. . $$p(D| theta) = displaystyle prod_{t in mathrm{obs}} p_t left(u_{[ mathrm{P}]}(t, theta) right) textrm{ where } p_t textrm{ is the probability density function of } mathrm{N}_{[ mathrm{P}]}(t) sim mathcal{N} left( mathrm{obs}_{[ mathrm{P}]}(t), , sigma cdot sqrt{ mathrm{obs}_{[ mathrm{P}]}(t) } right)$$ . Which leaves us with a &quot;hyperparameter&quot; $ sigma$, which we have set arbitrarily above. . likelihood_dist = multivariate_normal(mean=observations.values[1:], cov=Ï * np.diag(sqrt(observations.values[1:]))) def likelihood_logpdf(ut): return likelihood_dist.logpdf(ut) . . Note: We will supply noiseless measurements to our inference algorithms. However, our inference procedures will assume noise in the measurements. . Metropolis-Hastings . We can now evaluate the prior $p( theta)$ and the likelihood $p(D| theta)$ of kinetic parameters $ theta = { kon, koff, kcat, kuncat }$. Those two distributions permit us to elaborate an Markov Chain Monte Carlo (MCMC) routine to sample from the posterior $p( theta|D) propto p(D| theta) cdot p( theta)$. The algorithm is as follows: . Repeat: . Draw kinetic parameters from the proposal distribution. | Integrate the system with the proposed kinetic parameters. | Evaluate the likelihood of the trajectory generated in step 2. | Accept/Reject the proposal by a Metropolis-Hastings criterion. | Append the current kinetic parameters to the Markov Chain. | Construct a proposal distribution around the current kinetic parameters. | Since the likelihood assigns most of the probability mass to a fairly narrow region of parameter space, most parameter sets have extremely low probability. In order to preserve some numerical stability, we log-transform the typical Metropolis-Hastings expressions. So typically $Ï_t = mathrm{likelihood _pdf}(u_t) cdot mathrm{prior _pdf}(Î¸_t)$ and the acceptance criterion is $ frac{Ï_{t+1}}{Ï_t} &gt; mathrm{rand}([0,1])$. In log space, the acceptance criterion becomes: $ log(Ï_{t+1}) - log(Ï_t) &gt; log( mathrm{rand}([0,1]))$ with $ log(Ï_t) = mathrm{likelihood _logpdf}(u_t) + mathrm{prior _logpdf}(Î¸_t)$. . def MH_MCMC(chain_length=1e3): Î¸t = sample_prior() ut = simulate_measurements(exp_params(Î¸t)) Ït = likelihood_logpdf(ut) + prior_logpdf(**Î¸t) if all(ut == 0): return MH_MCMC(chain_length) cov = np.eye(4) * 5e-4 i = 0 accept_ratio = 0 chain = [] samples = [] while i &lt; chain_length: Î¸tp1 = proposal(Î¸t, cov) utp1 = simulate_measurements(exp_params(Î¸tp1)) Ïtp1 = likelihood_logpdf(utp1) + prior_logpdf(**Î¸tp1) if Ïtp1 - Ït &gt; np.log(np.random.rand()): Î¸t, ut, Ït = Î¸tp1, utp1, Ïtp1 accept_ratio += 1 chain.append(Î¸t) samples.append(ut) i += 1 if i % 100 == 0 and i &gt; 300: # cov = pd.DataFrame(chain[100:]).cov() print(i, end=&#39; r&#39;) chain = pd.DataFrame(chain) samples = pd.DataFrame(np.hstack((np.zeros((len(chain), 1)), samples)), columns=observations.index) accept_ratio = accept_ratio/chain_length return chain, samples, accept_ratio . Our proposal density for the time being can be a simple isotropic gaussian around the current parameters. We could update our Gaussian&#39;s covariance over the course of sampling, but that proved to be unhelpful for this problem. . def proposal(Î¸t, cov): Î¼ = [Î¸t[&#39;k_on&#39;], Î¸t[&#39;k_off&#39;], Î¸t[&#39;k_cat&#39;], Î¸t[&#39;k_uncat&#39;]] Î¸tp1 = dict(zip([&#39;k_on&#39;, &#39;k_off&#39;, &#39;k_cat&#39;, &#39;k_uncat&#39;], np.random.multivariate_normal(Î¼, cov))) return Î¸tp1 . # define a few extra helpers called from our MCMC routine def exp_params(log_kinetic_params): return {name: exp(val) for name, val in log_kinetic_params.items()} def simulate_measurements(kinetic_params): u = integrate_scipy_full(kinetic_params) return (u.loc[measurement_times, &#39;P&#39;].ravel()[1:] if len(u) &gt; 0 else np.zeros(10)) . Now let&#39;s put it into practice: . chain_length = 1e3 chain, samples, accept_ratio = MH_MCMC(chain_length=chain_length) print(&#39;accept_ratio:&#39;, accept_ratio) . accept_ratio: 0.209 . # plotting functions and figure styles def fig_style_3(ax): for side in [&quot;right&quot;,&quot;top&quot;]: ax.spines[side].set_visible(False) ax.set_xlabel(&#39;chain&#39;, weight=&#39;bold&#39;) ax.set_ylabel(&#39;log parameter values&#39;, weight=&#39;bold&#39;) def plot_chain(chain, ax=None): &#39;&#39;&#39;Note: chain is expected to be in log-parameter space (natural log)&#39;&#39;&#39; if ax is None: fig, ax = plt.subplots() chain.plot.line(xlim=(0,len(chain)), color=[c[param_name] for param_name in chain.columns], ax=ax) for param_name in chain.columns: param_value = default_kinetic_params[param_name] ax.axhline(np.log(param_value), lw=0.5, color=c[param_name], linestyle=&#39;--&#39;) ax.fill_between(np.arange(len(chain)), chain[param_name], np.repeat(np.log(param_value), len(chain)), color=c[param_name], alpha=0.05) fig_style_3(ax) def plot_samples(samples, ax=None): if ax is None: fig, ax = plt.subplots() alpha = max(1/np.sqrt(len(samples)), 0.1) observations.plot.line(marker=&#39;o&#39;, lw=0, color=c[&#39;P&#39;], ylim=(-300, 10800), ax=ax, legend=True) samples.T.plot.line(colormap=plt.get_cmap(&#39;plasma&#39;), alpha=alpha, ax=ax, legend=False, zorder=1) fig_style(ax) . plot_chain(chain) . # Marginal densities of parameter sets visited by the Markov Chain sns.pairplot(chain, kind=&quot;kde&quot;) HTML(resize_fig(600, 600)) . plot_samples(samples) . # define MCMC_run(), which runs MCMC and visualizes the result in one step. def MCMC_run(): chain, samples, accept_ratio = MH_MCMC(chain_length=chain_length) fig, axs = plt.subplots(1, 2) plot_chain(chain, ax=axs[0]) plot_samples(samples, ax=axs[1]) print(&#39;accept_ratio:&#39;, accept_ratio) return chain, samples, accept_ratio . chain_1, samples_1, accept_ratio_1 = MCMC_run() . accept_ratio: 0.288 . chain_2, samples_2, accept_ratio_2 = MCMC_run() . accept_ratio: 0.285 . chain_3, samples_3, accept_ratio_3 = MCMC_run() . accept_ratio: 0.302 . The above chains tell us something interesting: it appears to be possible to closely fit the observed data with very different parameter sets than the ones used to generate the observed trajectory -- except for $k_{ mathrm{cat}}$, which must be accurate. There must exist some subspace of parameters which yield similar $[ mathrm{P}](t)$ curves. We&#39;ll explore the topic of parameter identifiability in a future blog post. . Frequentist Approach: Inference by Optimization . In the previous section, we conducted a random walk in parameter space, biasing our random walk towards regions of the parameter space where both the prior probability and likelihood were greater. After a certain number of samples (the &quot;burn in&quot; phase), the samples from our random walk could be said to be drawn from the (exact) posterior distribution over the kinetic parameters. . An alternative approach begins with another premise: . Suppose we want to incorporate no prior knowledge, and let the timeseries data alone govern our determination of our enzyme&#39;s kinetic parameters. | Suppose as well that instead of searching for a distribution of plausible parameters, we&#39;re only interested in finding the single most likely set of parameters. | . These two choices recast the inference task as an optimization problem. We&#39;ll explore two approaches to optimize the kinetic parameters to fit the observed data. . Forward Sensitivities . Optimization problems require an objective, such as minimizing a loss (or cost) function. Let&#39;s use the conventional squared error between our observations $ mathrm{obs}(t)$ and our trajectory $u(t, theta)$ at the observed timepoints, scaled in accordance with the basic noise model we described in Â§3.1.2: $G(u(t, theta)) = displaystyle sum_{t in mathrm{obs}} left( frac{ mathrm{obs}(t) - u(t, theta)}{ sigma cdot sqrt{ mathrm{obs}(t)}} right)^2$. . # plot intuition for squared loss definition plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] ax = observations.rename(&#39;observations d(t)&#39;).plot.line(marker=&#39;o&#39;, lw=0, color=color([&#39;P&#39;]), legend=True, markersize=5, ylim=(-5e2,1e4), title=&#39;Intuition for the Squared Error Loss&#39;) (observations * 1.1).rename(&#39;simulation u(t)&#39;).plot.line(marker=&#39;o&#39;, lw=0.4, color=plt.get_cmap(&#39;plasma&#39;)(0.8), legend=True, markersize=3) # total_err = 0 for x, y in observations.iloc[1:].items(): rect = patches.Rectangle((x, y), y*0.1/1e4*0.225, y*0.1, linewidth=1, edgecolor=&#39;r&#39;, facecolor=&#39;mistyrose&#39;, lw=0.3) ax.add_patch(rect) fig_style(ax) . def loss(u): if not all(observations.index.isin(u.index)): return np.nan return sum(((observations - u.loc[observations.index, &#39;P&#39;])/(Ï * np.sqrt(observations))).dropna()**2) . In order to optimize $ theta$ with respect to to our loss function $G$, we need a means to evaluate the gradient of the loss with respect to the parameters. Recall the gradient is the vector of first derivatives of $G$ with respect to each parameter $ theta_i$. The simplest way to compute each entry of the gradient ($ frac{dG(u(t, theta))}{d theta_i}$) would be to evaluate $G$ slightly above and below $ theta_i$ to numerically approximate the derivative. Concatenating those (for each $ theta_i$) would yield a numerical approximation of the gradient vector. This technique is called Finite Differences, and it&#39;s valid, though impractical. We&#39;ll implement it to check the validitiy of gradients we compute by other means. . def finite_differences(_, Î¸): grad = {} for k,v in Î¸.items(): Ïµ = np.sqrt(v * 1e-15) grad[k] = (loss(integrate_scipy_full({**Î¸, **{k:v+Ïµ}})) - loss(integrate_scipy_full({**Î¸, **{k:v-Ïµ}}))) / (2*Ïµ) return grad . Beyond Finite Differences, we might try to analytically derive an expression for the gradient: . $$ frac{d}{d theta_i}G(u(t, theta)) = frac{d}{d theta_i} sum_{t in mathrm{obs}} left( frac{ mathrm{obs}(t) - u(t, theta)}{ sigma cdot sqrt{ mathrm{obs}(t)}} right)^2 = sum_{t in mathrm{obs}} left[ 2 left( frac{ mathrm{obs}(t) - u(t, theta)}{( sigma cdot sqrt{ mathrm{obs}(t)})^2} right) cdot frac{-du(t, theta)}{d theta_i} right]$$However, the quantity $ frac{du(t, theta)}{d theta_i}$ (called the sensitivity of the solution to a parameter) is not immediately available. We can derive it as follows: . Our original differential equation is $ frac{du(t, theta)}{dt} = f(u(t, theta), theta)$. If we take $ frac{ partial}{ partial theta_i} left[ frac{du(t, theta)}{dt} right] = frac{ partial}{ partial theta_i} left[ f(u(t, theta), theta) right]$, we can rearrange as $ frac{d}{dt} left[ frac{ partial u(t, theta)}{ partial theta_i} right] = frac{ partial}{ partial theta_i} left[ f(u(t, theta), theta) right]$ and then integrate over $t$ for . $$ int_{t_0}^T frac{d}{dt} left[ frac{ partial u(t, theta)}{ partial theta_i} right] mathrm{d}t = int_{t_0}^T frac{ partial}{ partial theta_i} left[ f(u(t, theta), theta) right] mathrm{d}t = int_{t_0}^T left[ frac{ partial f}{ partial u} Big|_{u(t, theta), theta} cdot frac{ partial u}{ partial theta_i} + frac{ partial f}{ partial theta_i} Big|_{u(t, theta), theta} right] mathrm{d}t$$What we&#39;ve done is define an ODE whose solution (integral) is that missing quantity, the sensitivity $ frac{du(t, theta)}{d theta_i}$. This ODE is aptly named the forward sensitivity ODE. We can integrate both the original ODE and the sensitivity ODE forwards in time together from $t_0$ to $T$. . But first, we need to understand the constituent expressions: $ frac{ partial f}{ partial u} Big|_{u(t, theta), theta}$ , $ frac{ partial u}{ partial theta_i}$ and $ frac{ partial f}{ partial theta_i} Big|_{u(t, theta), theta}$ . Recall, . $$ frac{du}{dt} = frac{d}{dt} begin{bmatrix}[ mathrm{S}] [ mathrm{E}] [ mathrm{ES}] [ mathrm{P}] end{bmatrix} = begin{bmatrix} k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{bmatrix} = f(u(t, theta), theta)$$$ frac{ partial f}{ partial u} Big|_{u(t, theta), theta}$ is the derivative of the derivative $f$ with respect to the state $u$. Since both the state $u$ and its derivative $f$ are 4D, this quantity is a 4x4 Jacobian: . $$ frac{df}{du} = begin{bmatrix} frac{df}{[ mathrm{S}]} &amp; frac{df}{[ mathrm{E}]} &amp; frac{df}{[ mathrm{ES}]} &amp; frac{df}{[ mathrm{P}]} end{bmatrix} = begin{bmatrix} -k_{ mathrm{on}}[ mathrm{E}] &amp; -k_{ mathrm{on}}[ mathrm{S}] &amp; k_{ mathrm{off}} &amp; 0 -k_{ mathrm{on}}[ mathrm{E}] &amp; -k_{ mathrm{on}}[ mathrm{S}] - k_{ mathrm{uncat}}[ mathrm{P}] &amp; k_{ mathrm{off}} + k_{ mathrm{cat}} &amp; -k_{ mathrm{uncat}}[ mathrm{E}] k_{ mathrm{on}}[ mathrm{E}] &amp; k_{ mathrm{on}}[ mathrm{S}] + k_{ mathrm{uncat}}[ mathrm{P}] &amp; -k_{ mathrm{off}} - k_{ mathrm{cat}} &amp; k_{ mathrm{uncat}}[ mathrm{E}] 0 &amp; -k_{ mathrm{uncat}}[ mathrm{P}] &amp; k_{ mathrm{cat}} &amp; -k_{ mathrm{uncat}}[ mathrm{E}] end{bmatrix}$$ def f_u_Jacobian(S, E, ES, P, k_on, k_off, k_cat, k_uncat): return np.array([ [-k_on * E, -k_on * S, k_off, 0], [-k_on * E, -k_on * S - k_uncat * P, k_off + k_cat, -k_uncat * E], [k_on * E, k_on * S + k_uncat * P, -k_off - k_cat, k_uncat * E], [0, -k_uncat * P, k_cat, -k_uncat * E] ]) . $ frac{ partial f}{ partial theta_i} Big|_{u(t, theta), theta}$ is the derivative of the derivative $f$ with respect to one of the parameters $ theta_i$. . $$ frac{ partial f}{ partial k_{ mathrm{on}}} = begin{bmatrix} -[ mathrm{E}][ mathrm{S}] -[ mathrm{E}][ mathrm{S}] [ mathrm{E}][ mathrm{S}] 0 end{bmatrix}, qquad frac{ partial f}{ partial k_{ mathrm{off}}} = begin{bmatrix} [ mathrm{ES}] [ mathrm{ES}] -[ mathrm{ES}] 0 end{bmatrix}, qquad frac{ partial f}{ partial k_{ mathrm{cat}}} = begin{bmatrix} 0 [ mathrm{ES}] -[ mathrm{ES}] [ mathrm{ES}] end{bmatrix}, qquad frac{ partial f}{ partial k_{ mathrm{uncat}}} = begin{bmatrix} 0 -[ mathrm{E}][ mathrm{P}] [ mathrm{E}][ mathrm{P}] -[ mathrm{E}][ mathrm{P}] end{bmatrix}, qquad $$ def f_k_on(S, E, ES, P): return np.array([[-E*S, -E*S, E*S, 0]]).T def f_k_off(S, E, ES, P): return np.array([[ES, ES, -ES, 0]]).T def f_k_cat(S, E, ES, P): return np.array([[0, ES, -ES, ES]]).T def f_k_uncat(S, E, ES, P): return np.array([[0, -E*P, E*P, -E*P]]).T cols = [v+u for u in [&#39;&#39;, &#39;_k_on&#39;, &#39;_k_off&#39;, &#39;_k_cat&#39;, &#39;_k_uncat&#39;] for v in [&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]] . $ frac{ partial u}{ partial theta_i}$ is the variable of integration, which means we only need to define a boundary condition for it, in this case, an initial value: . $$ frac{ partial u}{ partial theta_i} Big|_{t_0} = frac{ partial}{ partial theta_i} u(0, theta) $$ . But since in our case, our fixed initial condition $u(0, theta)$ does not depend on $ theta$, $ frac{ partial u}{ partial theta_i} Big|_{t_0} = 0$. . Now we&#39;re ready to augment our original Euler method to compute both $ int_{t_0}^T frac{du(t, theta)}{dt} mathrm{d}t$ as before and add $ int_{t_0}^T frac{ partial}{ partial theta_i} left[ f(u(t, theta), theta) right] mathrm{d}t$. . # define integrate_euler_full_sensitivities(), which integrates the full kinetics and sensitivity ODE with Euler&#39;s Method def integrate_euler_full_sensitivities(kinetic_params, dt=dt, steps=steps, initial_conditions=default_initial_conditions): k_on, k_off, k_cat, k_uncat, k_ms, k_mp = kinetic_params.values() S, E, ES, P = initial_conditions.values() u_k_on = np.zeros((4,1)) u_k_off = np.zeros((4,1)) u_k_cat = np.zeros((4,1)) u_k_uncat = np.zeros((4,1)) traj = [[S, E, ES, P, *u_k_on.flatten(), *u_k_off.flatten(), *u_k_cat.flatten(), *u_k_uncat.flatten()]] for i in range(int(steps)): S += (k_off * ES - k_on * E * S) * dt E += (k_off * ES - k_on * E * S + k_cat * ES - k_uncat * E * P) * dt ES += (k_on * E * S - k_off * ES - k_cat * ES + k_uncat * E * P) * dt P += (k_cat * ES - k_uncat * E * P) * dt f_u = f_u_Jacobian(S, E, ES, P, k_on, k_off, k_cat, k_uncat) u_k_on += (f_u @ u_k_on + f_k_on(S, E, ES, P)) * dt u_k_off += (f_u @ u_k_off + f_k_off(S, E, ES, P)) * dt u_k_cat += (f_u @ u_k_cat + f_k_cat(S, E, ES, P)) * dt u_k_uncat += (f_u @ u_k_uncat + f_k_uncat(S, E, ES, P)) * dt traj.append([S, E, ES, P, *u_k_on.T[0].copy(), *u_k_off.T[0].copy(), *u_k_cat.T[0].copy(), *u_k_uncat.T[0].copy()]) return pd.DataFrame(traj, columns=cols, index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)) . # integrate system ODE and sensitivity ODE together, and plot the trajectories start = time.process_time() traj_euler_full_sensitivities = integrate_euler_full_sensitivities(default_kinetic_params) euler_time = time.process_time() - start ODE_columns = traj_euler_full_sensitivities.columns[:4] sensitivity_columns = traj_euler_full_sensitivities.columns[4:] P_sensitivity_columns = traj_euler_full_sensitivities.columns[7::4] fig, axs = plt.subplots(1, 2) traj_euler_full_sensitivities[ODE_columns].plot.line(ax=axs[0], color=color(ODE_columns)) traj_euler_full_sensitivities[sensitivity_columns].plot.line(ax=axs[1], color=color(sensitivity_columns.str.split(&#39;_&#39;).str[0])) fig_style(axs[0]) fig_style(axs[1]) axs[1].set_ylabel(&#39;Sensitivity dÎ¸/dt&#39;) plt.tight_layout() . Unfortunately, as before, our simple-minded python code, although conceptually helpful, is too slow to use repeatedly inside a loop. Let&#39;s once again re-structure this code for scipy. . # define integrate_scipy_full_sensitivities (and helpers), which integrates the full kinetics and sensitivities with scipy def dy_full_sensitivities(t, y, k_on, k_off, k_cat, k_uncat, *args): # Y ordered S,E,ES,P dy = np.zeros(20) dy[0] = k_off * y[2] - k_on * y[1] * y[0] dy[1] = k_off * y[2] - k_on * y[1] * y[0] + k_cat * y[2] - k_uncat * y[1] * y[3] dy[2] = k_on * y[1] * y[0] - k_off * y[2] - k_cat * y[2] + k_uncat * y[1] * y[3] dy[3] = k_cat * y[2] - k_uncat * y[1] * y[3] f_u = f_u_Jacobian(*y[0:4], k_on, k_off, k_cat, k_uncat) dy[4:8] = np.dot(f_u, y[4:8]) + f_k_on(*y[0:4]).T dy[8:12] = np.dot(f_u, y[8:12]) + f_k_off(*y[0:4]).T dy[12:16] = np.dot(f_u, y[12:16]) + f_k_cat(*y[0:4]).T dy[16:20] = np.dot(f_u, y[16:20]) + f_k_uncat(*y[0:4]).T return dy def integrate_scipy_full_sensitivities(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps, atol=1e-8): t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = list(initial_conditions.values()) + [0]*16 try: sol = solve_ivp(dy_full_sensitivities, t_span, y0, args=(*kinetic_params.values(),), t_eval=t_eval, first_step=dt, method=&#39;LSODA&#39;, atol=atol) return pd.DataFrame(sol.y.T, index=sol.t, columns=cols) except: return pd.DataFrame(columns=cols) . # benchmark our naive code against scipy&#39;s integrator for the sensitivity equations start = time.process_time() traj_scipy_full_sensitivities = integrate_scipy_full_sensitivities(default_kinetic_params) scipy_time = time.process_time() - start ax = traj_euler_full_sensitivities[sensitivity_columns].plot.line(color=color(sensitivity_columns.str.split(&#39;_&#39;).str[0]), alpha=0.5, legend=False) traj_scipy_full_sensitivities[sensitivity_columns].plot.line(ax=ax, color=color(sensitivity_columns.str.split(&#39;_&#39;).str[0]), linestyle=&#39;--&#39;, legend=False) ax.set_ylabel(&#39;Sensitivity dÎ¸/dt&#39;, weight=&#39;bold&#39;) ax.set_xlabel(&#39;time (s)&#39;, weight=&#39;bold&#39;) print(f&#39;Our naÃ¯ve code takes {round(euler_time, 2)}s, whereas the optimized scipy code takes {round(scipy_time, 4)}s to generate the same trajectory.&#39;) . Our naÃ¯ve code takes 22.08s, whereas the optimized scipy code takes 0.1982s to generate the same trajectory. . Recall, computing the sensitivity of the solution with respect to the parameters $ frac{du(t, theta)}{d theta_i}$ was in service of computing the gradient of our loss function with respect to the parameters: . $$ frac{dG(u(t, theta))}{d theta_i} = sum_{t in mathrm{obs}} left[ 2 left( frac{ mathrm{obs}(t) - u(t, theta)}{( sigma cdot sqrt{ mathrm{obs}(t)})^2} right) cdot frac{-du(t, theta)}{d theta_i} right]$$Now, since we set up this problem such that we only observe $ mathrm{obs}_{[ mathrm{P}]}(t)$, we are only able to compare the integrated kinetics of $ u_{[ mathrm{P}]}(t, theta)$ and so our gradient expression becomes: . $$ frac{dG(u(t, theta))}{d theta_i} = sum_{t in mathrm{obs}} left[ 2 left( frac{( mathrm{obs}_{[ mathrm{P}]}(t) - u_{[ mathrm{P}]}(t, theta))}{( sigma cdot sqrt{ mathrm{obs}_{[ mathrm{P}]}(t)})^2} right) cdot frac{ -d u_{[ mathrm{P}]}(t, theta)}{d theta_i} right]$$ . # define gradient_of_loss() which returns the gradient of the loss with respect to each parameter def gradient_of_loss_via_sensitivities(integrated_system_and_sensitivities, _): diff = 2*(observations - integrated_system_and_sensitivities.loc[observations.index, &#39;P&#39;]) / (Ï * np.sqrt(observations))**2 P_k = -integrated_system_and_sensitivities.loc[observations.index, P_sensitivity_columns] grad = P_k.multiply(diff, axis=&#39;rows&#39;).sum() grad.index = grad.index.str.lstrip(&#39;P_&#39;) return grad.to_dict() . We notice that $ frac{ d u_{[ mathrm{P}]}(t, theta)}{dk_{ mathrm{uncat}}}$ reaches $O(10^6)$. This difference of the scales of the sensitivities may be because the parameters span many orders of magnitude, so the response of the system to small perturbations in some parameters may be much greater than others, especially integrated over time. . # plot the subset of sensitivities which impinge on [P] -- notice the scale of dP/dk_uncat! ax = traj_scipy_full_sensitivities[P_sensitivity_columns].plot.line(color=color(P_sensitivity_columns.str.split(&#39;_&#39;, 1).str[1])) ax.set_xlabel(&#39;time (s)&#39;, weight=&#39;bold&#39;) ax.set_ylabel(&#39;Sensitivity dÎ¸/dt&#39;, weight=&#39;bold&#39;) None . We&#39;ve set ourselves the task of optimizing the 4 ODE parameters to minimize the squared error with respect to an observed timeseries. We need somewhere to begin optimization from. Let&#39;s use the means of the prior distributions (from Â§3.1.1) for each parameter as a starting point. . Î¸_0 = {&#39;k_on&#39;: exp(log_kon_normal.mean()), &#39;k_off&#39;: exp(log_koff_normal.mean()), &#39;k_cat&#39;: exp(log_kcat_normal.mean()), &#39;k_uncat&#39;: exp(log_kuncat_normal.mean())} . Our gradient descent routine iterates a loop: . Integrate the system ODE and sensitivity ODE with the current parameters. | Compute the gradient of the loss with the current parameters. | Update the parameters with a gradient step | Our optimization routine will use one extra trick: a momentum term. This amounts to updating the gradient step expression from . $$ theta_{t+1} = theta_t - eta cdot frac{dG(u(t, theta))}{d theta} qquad mathrm{to} qquad begin{aligned} v_{t+1} &amp;= gamma cdot v_t + frac{dG(u(t, theta))}{d theta} theta_{t+1} &amp;= theta_t - eta cdot v_t end{aligned}$$This expression has two hyperparamters: $ eta$, the learning rate and $ gamma$, the momentum parameter. We&#39;ll set $ eta = 0.01$ at first and decrease it as we converge towards a minimum. We set $ gamma$ to the typical 0.9. . # define optimize_by_gradient_descent Optimization_Record = namedtuple(&#39;Optimization_Record&#39;, [&#39;Î¸&#39;, &#39;u&#39;, &#39;loss&#39;, &#39;G_Î¸&#39;]) def optimize_by_gradient_descent(Î¸_0, integrate=integrate_scipy_full_sensitivities, gradient=gradient_of_loss_via_sensitivities): &#39;&#39;&#39; signatures: - integrate(Î¸_t) - gradient(u_t, Î¸_t) &#39;&#39;&#39; Î¸_t = dict(Î¸_0) v_t = {k: 0 for k in Î¸_0.keys()} u = integrate(Î¸_t) curr_loss = loss(u) Î· = 1e-2 Î³ = 0.9 Î¸_record = [Î¸_t] v_record = [v_t] u_record = [u] loss_record = [curr_loss] grad_record = [] while Î· &gt;= 1e-30: G_Î¸, *details = gradient(u, Î¸_t) v_t = {k: Î³ * v_t[k] + G_Î¸[k] for k in v_t.keys()} Î¸_t = {k: max(Î¸_t[k] - Î· * v_t[k], 0) for k in Î¸_t.keys()} next_u = integrate(Î¸_t) next_loss = loss(next_u) if next_loss &lt; curr_loss: u = next_u curr_loss = next_loss v_record.append(v_t) Î¸_record.append(Î¸_t) u_record.append(u) loss_record.append(curr_loss) grad_record.append(G_Î¸) clear_output(wait=True) print(Î¸_0, &#39; n tLoss: &#39;, np.round(curr_loss, 6), &#39; -&gt; &#39;, np.round(next_loss, 6), &#39; n t|Gradient|: &#39;, np.linalg.norm(list(G_Î¸.values())), &#39; n t|v_t|: &#39;, np.linalg.norm(list(v_t.values())), &#39; n tÎ·: &#39;, Î·, &#39; n t&#39;, Î¸_t) else: Î· = Î·/np.sqrt(10) # print(&#39; tÎ·: &#39;, Î·) v_t = {k: 0 for k in Î¸_0.keys()} Î¸_t = Î¸_record[-1] continue Î¸_record = pd.DataFrame(Î¸_record) loss_record = pd.Series(loss_record) grad_record = pd.DataFrame(grad_record) return Optimization_Record(Î¸=Î¸_record, u=u_record, loss=loss_record, G_Î¸=grad_record) . optimization_record = optimize_by_gradient_descent(Î¸_0) . {&#39;k_on&#39;: 1.0489415890739007, &#39;k_off&#39;: 9.301874926576735, &#39;k_cat&#39;: 26.789395694394965, &#39;k_uncat&#39;: 0.8975846605968499} Loss: 2.135936 -&gt; 2.135936 |Gradient|: 1.756622982117719 |v_t|: 1.756622982117719 Î·: 3.1622776601683754e-12 {&#39;k_on&#39;: 3.6718623703555924, &#39;k_off&#39;: 9.116898206276698, &#39;k_cat&#39;: 27.228235334781886, &#39;k_uncat&#39;: 0.7150114110837106} . # plot result of optimization, starting from mean parameter values def plot_optimization_trajectory(optimization_record): plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] fig, axs = plt.subplots(1, 2) num_iterations = len(optimization_record.u) alpha = max(1/np.sqrt(num_iterations), 0.1) for i, u_t in enumerate(optimization_record.u): u_t[&#39;P&#39;].plot.line(ax=axs[0], legend=False, zorder=1, c=plt.get_cmap(&#39;plasma&#39;)(i/num_iterations), alpha=alpha) observations.plot.line(marker=&#39;o&#39;, lw=0, color=c[&#39;P&#39;], ylim=(-300, 10800), ax=axs[0], legend=True) fig_style(axs[0]) optimization_record.loss.plot.line(ax=axs[1], ylim=(0, optimization_record.loss.max()*1.1)) final_loss = optimization_record.loss.iloc[-1] axs[1].text(0, final_loss+optimization_record.loss.max()/50, str(np.round(final_loss, 3)), c=&#39;red&#39;) axs[1].axhline(final_loss, c=&#39;r&#39;, linestyle=&#39;--&#39;, lw=0.5) axs[1].xaxis.set_major_locator(mtick.MaxNLocator(integer=True)) axs[1].set_xlabel(&#39;iteration&#39;, weight=&#39;bold&#39;) axs[1].set_ylabel(&#39;Loss&#39;, weight=&#39;bold&#39;) for side in [&quot;right&quot;,&quot;top&quot;]: axs[1].spines[side].set_visible(False) . plot_optimization_trajectory(optimization_record) . We said initially we didn&#39;t want to rely on prior knowledge of typical enzyme parameters in the sensitivity-based approach, but we&#39;ve used them above to define our starting point for optimization, $ theta_0$. In many settings, we may not have a good estimate of where to begin from, and the conventional approach is to try multiple seeds. Let&#39;s try outlandishly wrong starting points, and see whether our gradient descent routine is able to find parameters which fit the data. . # define procedure to sample outlandish points, and run batch optimization from each of those seeds def prior_samples(n): prior_draws = [] while len(prior_draws) &lt; n: draw = sample_prior() if all([(np.linalg.norm(np.array(list(draw.values())) - np.array(list(prior_draw.values()))) &gt; 1) for prior_draw in prior_draws]): prior_draws.append(draw) return prior_draws def optimize_parametersets(parametersets): optimization_records = [optimize_by_gradient_descent(Î¸_0) for Î¸_0 in parametersets] os.system(&quot;printf &#39; a&#39;&quot;) return optimization_records optimization_runs_pickle_path = Path(&#39;../data/Enzyme_Kinetic_Parameter_Inference/optimization_runs.pickle&#39;) def get_optimization_runs(): if optimization_runs_pickle_path.is_file(): with optimization_runs_pickle_path.open(&#39;rb&#39;) as f: optimization_records = pickle.load(f) # some sort of QC here? return optimization_records return None def compute_optimization_runs(): draws = prior_samples(10) optimization_records = optimize_parametersets([exp_params(draw) for draw in draws]) with optimization_runs_pickle_path.open(&#39;wb&#39;) as f: pickle.dump(optimization_records, f) os.system(&quot;printf &#39; a&#39;&quot;) return optimization_records def get_or_compute_optimization_runs(): return get_optimization_runs() or compute_optimization_runs() . optimization_records = get_or_compute_optimization_runs() . # Plot results of optimizations of 10 seeds: Loss trajectories and |gradient| trajectories def plot_optimization_records_losses_and_magnitudes(optimization_records, until=2000): plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] fig, axs = plt.subplots(1, 2) optimization_records_losses = pd.DataFrame([optimization_record.loss for optimization_record in optimization_records]).T.rename_axis(&#39;iteration&#39;) optimization_records_losses.plot.line(ax=axs[0], xlim=(0, until), logy=True, title=&#39;Loss trajectories for 10 draws from the prior&#39;).set_ylabel(&#39;log(Loss)&#39;, fontdict={&#39;weight&#39;:&#39;bold&#39;}) optimization_records_gradient_magnitudes = pd.DataFrame([np.linalg.norm(r.G_Î¸, axis=1) for r in optimization_records]).T.rename_axis(&#39;iteration&#39;) optimization_records_gradient_magnitudes.plot.line(ax=axs[1], xlim=(0, until), logy=True, title=&#39;Gradient magnitudes for 10 draws from the prior&#39;).set_ylabel(&#39;log(|gradient|)&#39;, fontdict={&#39;weight&#39;:&#39;bold&#39;}) plot_optimization_records_losses_and_magnitudes(optimization_records) . As we did not add noise to our measurements, a loss of 0 is possible. However, none of our optimizations reach 0 loss. We would anticipate the magnitude of our gradients to approach 0 as well, as optimization proceeds successfully. Let&#39;s dig into these optimization runs a little further... . # define plot_optimization_trajectories() def plot_optimization_trajectories(optimization_records, parameter_trajectory_is_in_log_space=False): plt.rcParams[&#39;figure.figsize&#39;] = [12, 4*len(optimization_records)] fig, axs = plt.subplots(len(optimization_records), 2) for i, record in enumerate(optimization_records): P_traj = pd.concat([df[&#39;P&#39;] for df in record.u], axis=1) P_traj.columns = range(len(P_traj.columns)) plot_samples(P_traj.T, ax=axs[i][0]) axs[i][0].set_title(f&#39;prior draw {i}&#39;, fontdict={&#39;weight&#39;:&#39;bold&#39;}).set_c(matplotlib_colors[i]) if parameter_trajectory_is_in_log_space: plot_chain(record.Î¸, ax=axs[i][1]) else: plot_chain(np.log(record.Î¸), ax=axs[i][1]) . # plot the optimization trajectories warnings.filterwarnings(&#39;ignore&#39;) plot_optimization_trajectories(optimization_records) plt.tight_layout() . Some of our optimizations succeed fit the data, whereas some fail to. Once again, even those that fit the data quite well (and therefore approach zero loss) do not recover the original parameters the trajectory was generated with. This result warrants a closer inspection of the loss surface. . Visualizing $G(u(t, theta))$, which is a $ R^4 mapsto R^1$ function, is tricky, but we can get a feel for it by examining orthogonal 2D slices. . First, we need to evaluate the loss everywhere on a grid. . # define functions to evaluate the loss on each point on a grid plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] warnings.filterwarnings(&#39;once&#39;) Î¸_opt = {key: val for key, val in default_kinetic_params.items() if key in [&#39;k_on&#39;, &#39;k_off&#39;, &#39;k_cat&#39;, &#39;k_uncat&#39;]} def parameter_loss(k_on,k_off,k_cat,k_uncat): return loss(integrate_scipy_full({&#39;k_on&#39;:k_on,&#39;k_off&#39;:k_off,&#39;k_cat&#39;:k_cat,&#39;k_uncat&#39;:k_uncat})) loss_hypergrid_hdf5_path = Path(&#39;../data/Enzyme_Kinetic_Parameter_Inference/loss_hypergrid.hdf5&#39;) def get_or_compute_global_loss_hypergrid(): return get_loss_hypergrid(&#39;global_loss_hypergrid&#39;) or compute_global_loss_hypergrid() def get_or_compute_optimization_neighborhood_loss_hypergrid(Î¸_records, Î¸_opt): return get_loss_hypergrid(&#39;optimization_neighborhood_loss_hypergrid&#39;) or compute_optimization_neighborhood_loss_hypergrid(Î¸_records, Î¸_opt) def get_or_compute_optimum_neighborhood_loss_hypergrid(Î¸_opt): return get_loss_hypergrid(&#39;optimum_neighborhood_loss_hypergrid&#39;) or compute_point_neighborhood_loss_hypergrid(Î¸_opt) def get_loss_hypergrid(dset_name): if loss_hypergrid_hdf5_path.is_file(): with h5py.File(loss_hypergrid_hdf5_path, &#39;r&#39;) as h5f: dset = h5f.get(dset_name) loss_hypergrid = np.array(dset) if (loss_hypergrid.ndim and loss_hypergrid.size): # check non-empty indices = dset.attrs.get(&#39;indexes&#39;) return indices, loss_hypergrid return None def compute_loss_hypergrid(indices, name=None): loss_hypergrid = np.vectorize(parameter_loss)(*np.meshgrid(*indices, sparse=True)) if name: with h5py.File(loss_hypergrid_hdf5_path, &#39;a&#39;) as h5f: if name in h5f: print(name + &#39; already exists in file, overwriting...&#39;) del h5f[name] dset = h5f.create_dataset(name, data=loss_hypergrid) dset.attrs[&#39;index_names&#39;] = [&#39;k_on&#39;,&#39;k_off&#39;,&#39;k_cat&#39;,&#39;k_uncat&#39;] dset.attrs[&#39;indexes&#39;] = indices os.system(&quot;printf &#39; a&#39;&quot;) return loss_hypergrid def compute_global_loss_hypergrid(grid_size=11): x = np.logspace(-4, 6, grid_size) indices = [x, x, x, x] return indices, compute_loss_hypergrid(&#39;global_loss_hypergrid&#39;, indices) def compute_optimization_neighborhood_loss_hypergrid(Î¸_records, Î¸_opt, grid_size=15, name=&#39;optimization_neighborhood_loss_hypergrid&#39;): Min = [min([Î¸_record[k].min() for Î¸_record in Î¸_records] + [Î¸_opt[k]]) for k in Î¸_opt.keys()] Max = [max([Î¸_record[k].max() for Î¸_record in Î¸_records] + [Î¸_opt[k]]) for k in Î¸_opt.keys()] indices = [np.linspace(lower*0.9, upper*1.1, grid_size) for lower, upper in zip(Min, Max)] return indices, compute_loss_hypergrid(indices, name=name) def compute_point_neighborhood_loss_hypergrid(Î¸, grid_size=15, name=&#39;optimum_neighborhood_loss_hypergrid&#39;): indices = [np.linspace(p-p*0.8, p+p*0.8, grid_size) for p in Î¸.values()] return indices, compute_loss_hypergrid(indices, name=name) . global_indices, global_loss_hypergrid = get_or_compute_global_loss_hypergrid() global_loss_hypergrid.shape . (11, 11, 11, 11) . optimum_neighborhood_indices, optimum_neighborhood_loss_hypergrid = get_or_compute_optimum_neighborhood_loss_hypergrid(Î¸_opt) optimum_neighborhood_loss_hypergrid.shape . (15, 15, 15, 15) . # Define interactive loss landscape visualization def explore_loss_landscape(k_on, k_off, k_cat, k_uncat, scales, loss_hypergrid, Î¸_records, Î¸_opt, cmin=0, cmax=3000): params = Î¸_opt.keys() n = len(params) fig = make_subplots(rows=n-1, cols=n-1, shared_xaxes=True, shared_yaxes=True) for i, param1 in enumerate(params): for j, param2 in enumerate(params): if i &gt; j: z = np.squeeze(loss_hypergrid[None if &#39;k_on&#39; in [param1, param2] else k_on, None if &#39;k_off&#39; in [param1, param2] else k_off, None if &#39;k_cat&#39; in [param1, param2] else k_cat, None if &#39;k_uncat&#39; in [param1, param2] else k_uncat]).T # Contour fig.add_trace( go.Contour(z=z, x=scales[j], y=scales[i], coloraxis=&quot;coloraxis&quot;, contours_coloring=&#39;heatmap&#39;), row=i, col=j+1 ) # Optimum star if len(Î¸_opt.keys()): fig.add_trace( go.Scatter(x=[Î¸_opt[param2]], y=[Î¸_opt[param1]], mode=&#39;markers&#39;, marker=dict(symbol=&#39;star-dot&#39;, color=&#39;limegreen&#39;, line_color=&#39;black&#39;, line_width=0.2, size=12), showlegend=False, name=&quot;optimum&quot;), row=i, col=j+1 ) # Optimization trajectory lines if len(Î¸_records): for n, (Î¸_record, c) in enumerate(zip(Î¸_records, matplotlib_colors+[&#39;red&#39;]*4)): fig.add_trace( go.Scatter(x=Î¸_record[param2], y=Î¸_record[param1], mode=&#39;lines&#39;, line=dict(color=c), showlegend=False, name=f&quot;draw {n}&quot;), row=i, col=j+1 ) fig.add_trace( go.Scatter(x=[Î¸_record[param2].iloc[-1]], y=[Î¸_record[param1].iloc[-1]], mode=&#39;markers&#39;, marker=dict(symbol=&#39;diamond&#39;, color=c, size=5), showlegend=False), row=i, col=j+1 ) if i == len(params)-1: fig.update_xaxes(title_text=param2, row=3, col=j+1) if j == 0: fig.update_yaxes(title_text=param1, row=i, col=1) fig.update_layout(height=1000, width=1000, coloraxis=dict(colorscale=&#39;magma&#39;, reversescale=True, cmin=cmin, cmax=cmax)) # fig.show() return fig.to_html(include_plotlyjs=&#39;cdn&#39;, include_mathjax=False) def explore_global_loss_hypergrid(k_on, k_off, k_cat, k_uncat, scales, loss_hypergrid, log_Î¸_records, Î¸_opt): &#39;&#39;&#39;Note, this function expects `scales`, `Î¸_record`, and `Î¸_opt` to have been log&#39;d (base 10)&#39;&#39;&#39; k_on = int(np.log10(k_on)+4) k_off = int(np.log10(k_off)+4) k_cat = int(np.log10(k_cat)+4) k_uncat = int(np.log10(k_uncat)+4); return explore_loss_landscape(k_on, k_off, k_cat, k_uncat, scales, loss_hypergrid, log_Î¸_records, Î¸_opt) def explore_neighborhood_loss_hypergrid(k_on, k_off, k_cat, k_uncat, scales, loss_hypergrid, Î¸_records, Î¸_opt): k_on = np.where(scales[0] == k_on)[0][0] k_off = np.where(scales[1] == k_off)[0][0] k_cat = np.where(scales[2] == k_cat)[0][0] k_uncat = np.where(scales[3] == k_uncat)[0][0] return explore_loss_landscape(k_on, k_off, k_cat, k_uncat, scales, loss_hypergrid, Î¸_records, Î¸_opt) . We&#39;ll first explore the loss function on a grid in $ log_{10}$ space. This will give us a feel for the global shape of the loss surface, so long as we&#39;re mindful of the $ log_{10}$-distortion. We can also project the optimization trajectories onto each 2D slice (as well as the global optimum), while remembering that the trajectories are 4D, so their projections may not look appear to be optimizing the loss in each 2D slice. We can also take the opportunity to visualize the MCMC trajectories as a comparison (in red). . Note: Run the notebook to interactively explore this landscape with sliders that translate the slices through the 4D space. . # global view of optimization landscape sliders = [SelectionSlider(options=index, description=name, value=index[len(index)//2]) for name, index in zip([&#39;k_on&#39;,&#39;k_off&#39;,&#39;k_cat&#39;,&#39;k_uncat&#39;], global_indices)] w = interactive(explore_global_loss_hypergrid, k_on=sliders[0], k_off=sliders[1], k_cat=sliders[2], k_uncat=sliders[3], scales=fixed(np.log10(global_indices)), loss_hypergrid=fixed(global_loss_hypergrid), log_Î¸_records=fixed([np.log10(r.Î¸+1e-4) for r in optimization_records] + [np.log10(np.exp(c)) for c in (chain, chain_1, chain_2, chain_3)]), Î¸_opt=fixed({k: np.log10(v) for k,v in Î¸_opt.items()})) display(w) HTML(w.result) # fig.result.update_xaxes(range=[-4, 6], dtick=2) # fig.result.update_yaxes(range=[-4, 6], dtick=2) . . . . Note: The contour plots above suggest local optima, but those are artifacts of plotly&#8217;s contour function and the sparsity of our grid. To understand why our optimizations are failing to reach the optimum, we can also explore the loss around the optimum, or around the termination points of each optimization. Let&#39;s see the neighborhood of the optimum. . # local view of optimization landscape sliders = [SelectionSlider(options=index, description=name, value=index[len(index)//2]) for name, index in zip([&#39;k_on&#39;,&#39;k_off&#39;,&#39;k_cat&#39;,&#39;k_uncat&#39;], optimum_neighborhood_indices)] w = interactive(explore_neighborhood_loss_hypergrid, k_on=sliders[0], k_off=sliders[1], k_cat=sliders[2], k_uncat=sliders[3], scales=fixed(optimum_neighborhood_indices), loss_hypergrid=fixed(optimum_neighborhood_loss_hypergrid), Î¸_records=fixed([]), Î¸_opt=fixed(Î¸_opt)) display(w) HTML(w.result) . . . The optimum falls in a valley, which, once attained, likely has a very small gradient. This corresponds to the valley we saw in the $ log_{10}$-scaled global loss landscape above. Small gradients can be difficult to compute accurately with finite numerical precision, and our optimizer will stop once it&#39;s unable to distinguish which direction is downhill, because the loss surface is locally flat, and tiny steps in the direction of the approximate gradient do not decrease the loss. . Our parameters span many orders of magnitude. In our MCMC approach to this problem, we dealt with our kinetic parameters in log space. Let&#39;s take a quick detour and try gradient-based optimization for the log-parameters. . Forward Sensitivities in Log Space . Many of the expressions derived in the previous section remain the same when we optimize the loss $G$ with respect to the log-parameters. The Jacobian $ frac{ partial f}{ partial u} Big|_{u(t, theta), theta}$ remains the same, however, the $ frac{ partial f}{ partial theta} Big|_{u(t, theta), theta}$ terms are replaced by $ frac{ partial f}{ partial log( theta)} Big|_{u(t, theta), theta}$ terms: . $$ frac{ partial f}{ partial log(k_{ mathrm{on}})} = begin{bmatrix} -k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] -k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] 0 end{bmatrix}, qquad frac{ partial f}{ partial log(k_{ mathrm{off}})} = begin{bmatrix} k_{ mathrm{off}}[ mathrm{ES}] k_{ mathrm{off}}[ mathrm{ES}] -k_{ mathrm{off}}[ mathrm{ES}] 0 end{bmatrix}, qquad frac{ partial f}{ partial log(k_{ mathrm{cat}})} = begin{bmatrix} 0 k_{ mathrm{cat}}[ mathrm{ES}] -k_{ mathrm{cat}}[ mathrm{ES}] k_{ mathrm{cat}}[ mathrm{ES}] end{bmatrix}, qquad frac{ partial f}{ partial log(k_{ mathrm{uncat}})} = begin{bmatrix} 0 -k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] -k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{bmatrix}, qquad $$ def f_log_k_on(S, E, ES, P, k_on): return np.array([[-k_on*E*S, -k_on*E*S, k_on*E*S, 0]]).T def f_log_k_off(S, E, ES, P, k_off): return np.array([[k_off*ES, k_off*ES, -k_off*ES, 0]]).T def f_log_k_cat(S, E, ES, P, k_cat): return np.array([[0, k_cat*ES, -k_cat*ES, k_cat*ES]]).T def f_log_k_uncat(S, E, ES, P, k_uncat): return np.array([[0, -k_uncat*E*P, k_uncat*E*P, -k_uncat*E*P]]).T . We&#39;ll skip straight to the scipy implementation. . # define integrate_scipy_log_sensitivities (and helpers), which integrates the full kinetics and sensitivities with respect to the log&#39;d parameters with scipy def dy_log_sensitivities(t, y, k_on, k_off, k_cat, k_uncat, *args): # Y ordered S,E,ES,P dy = np.zeros(20) dy[0] = k_off * y[2] - k_on * y[1] * y[0] dy[1] = k_off * y[2] - k_on * y[1] * y[0] + k_cat * y[2] - k_uncat * y[1] * y[3] dy[2] = k_on * y[1] * y[0] - k_off * y[2] - k_cat * y[2] + k_uncat * y[1] * y[3] dy[3] = k_cat * y[2] - k_uncat * y[1] * y[3] f_u = f_u_Jacobian(*y[0:4], k_on, k_off, k_cat, k_uncat) dy[4:8] = np.dot(f_u, y[4:8]) + f_log_k_on(*y[0:4], k_on).T dy[8:12] = np.dot(f_u, y[8:12]) + f_log_k_off(*y[0:4], k_off).T dy[12:16] = np.dot(f_u, y[12:16]) + f_log_k_cat(*y[0:4], k_cat).T dy[16:20] = np.dot(f_u, y[16:20]) + f_log_k_uncat(*y[0:4], k_uncat).T return dy def integrate_scipy_log_sensitivities(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps): &#39;&#39;&#39;Note this function assumes the kinetic params are not in log space -- exp() them beforehand&#39;&#39;&#39; t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = list(initial_conditions.values()) + [0]*16 try: sol = solve_ivp(dy_log_sensitivities, t_span, y0, args=(*kinetic_params.values(),), t_eval=t_eval, first_step=dt, method=&#39;LSODA&#39;) return pd.DataFrame(sol.y.T, index=sol.t, columns=cols) except: return pd.DataFrame(columns=cols) . traj_scipy_log_sensitivities = integrate_scipy_log_sensitivities(default_kinetic_params) fig, axs = plt.subplots(1, 2) traj_scipy_log_sensitivities[ODE_columns].plot.line(ax=axs[0], color=color(ODE_columns)) traj_scipy_log_sensitivities[sensitivity_columns].plot.line(ax=axs[1], color=color(sensitivity_columns.str.split(&#39;_&#39;).str[0])) fig_style(axs[0]) fig_style(axs[1]) axs[1].set_ylabel(&#39;Sensitivity dÎ¸/dt&#39;) plt.tight_layout() . We&#39;ll once more start from the mean of the prior, . log_Î¸_0 = {&#39;k_on&#39;: log_kon_normal.mean(), &#39;k_off&#39;: log_koff_normal.mean(), &#39;k_cat&#39;: log_kcat_normal.mean(), &#39;k_uncat&#39;: log_kuncat_normal.mean()} . optimization_record = optimize_by_gradient_descent(log_Î¸_0, integrate=lambda log_Î¸: integrate_scipy_log_sensitivities(exp_params(log_Î¸))) . {&#39;k_on&#39;: 0.04778164537946772, &#39;k_off&#39;: 2.2302159848479026, &#39;k_cat&#39;: 3.288006126168655, &#39;k_uncat&#39;: -0.1080478337126154} Loss: 0.697513 -&gt; 0.697513 |Gradient|: 0.5847699040405534 |v_t|: 0.5847699040405534 Î·: 9.999999999999989e-11 {&#39;k_on&#39;: 1.1154074189761212, &#39;k_off&#39;: 1.754124991901947, &#39;k_cat&#39;: 3.3975347971126704, &#39;k_uncat&#39;: 0} . plot_optimization_trajectory(optimization_record) . This seems promising. Let&#39;s once more start from many initial points. . # Define functions to run multiple log-parameter optimizations log_optimization_runs_pickle_path = Path(&#39;../data/Enzyme_Kinetic_Parameter_Inference/log_optimization_runs.pickle&#39;) def optimize_log_parametersets(log_parametersets): optimization_records = [optimize_by_gradient_descent(log_Î¸_0, integrate=lambda log_Î¸: integrate_scipy_log_sensitivities(exp_params(log_Î¸))) for log_Î¸_0 in log_parametersets] os.system(&quot;printf &#39; a&#39;&quot;) return optimization_records def get_log_optimization_runs(): if log_optimization_runs_pickle_path.is_file(): with log_optimization_runs_pickle_path.open(&#39;rb&#39;) as f: log_optimization_records = pickle.load(f) # some sort of QC here? return log_optimization_records return None def compute_log_optimization_runs(): draws = prior_samples(10) optimization_records = [record for record in optimize_log_parametersets(draws) if record] with log_optimization_runs_pickle_path.open(&#39;wb&#39;) as f: pickle.dump(optimization_records, f) os.system(&quot;printf &#39; a&#39;&quot;) return optimization_records def get_or_compute_log_optimization_runs(): return get_log_optimization_runs() or compute_log_optimization_runs() . log_optimization_records = get_or_compute_log_optimization_runs() . plot_optimization_records_losses_and_magnitudes(log_optimization_records) . # plot the optimization trajectories warnings.filterwarnings(&#39;ignore&#39;) plot_optimization_trajectories(log_optimization_records, parameter_trajectory_is_in_log_space=True) plt.tight_layout() . Steps in log space can be quite large, often yielding much faster convergences. We see some trajectories still fail to approach optima. . Adjoint Method . There exists another way to evalate the gradient of the loss with respect to the parameters. Our intention is not to improve the accuracy of the gradients, but instead the asymptotic computational complexity of evaluating the gradient, and thus decrease the runtime for optimizing the parameters of systems with many parameters. . Our enzyme kinetics system has 4 parameters, which is not many, so we will not experience a notable speedup in our optimizations, but including the approach is perhaps easier to illustrate granularly for a small system. . In the Forward Sensitivities approach, a system of Ordinary Differential Equations in 4 variables with 4 parameters had us integrating 16 sensitivity variables: $ frac{ partial [ mathrm{X}_i]}{ partial theta_j}$ for each combination of $[ mathrm{X}_i]$ and $ theta_j$. The number of sensitivity ODE expressions scales as $O(| mathrm{X}| cdot | theta|)$. The Adjoint Method for ODEs will permit us to reduce that to $O(| mathrm{X}| + | theta|)$. Let&#39;s walk through the derivation. Fair warning: it&#39;s lengthy (at least by my standards). . Derivation: . We had chosen our loss to be . $$G(u(t, theta)) = sum_{t in mathrm{obs}} left( frac{ mathrm{obs}(t) - u(t, theta)}{ sigma cdot sqrt{ mathrm{obs}(t)}} right)^2 $$ Re-write $G$ as an integral of the instantaneous loss over continuous time, for which we can use dirac deltas $ delta$ . $$ begin{aligned}G(u(t, theta)) &amp;= int_{t_0}^T left[ left( frac{ mathrm{obs}(t) - u(t, theta)}{ sigma cdot sqrt{ mathrm{obs}(t)}} right)^2 cdot sum_{t_i in mathrm{obs}} delta(t_i-t) right] , mathrm{d}t &amp;= int_{t_{0}}^{T}g(u(t, theta)) , mathrm{d}t end{aligned}$$ We want to specify the constraint that $ frac{du}{ , mathrm{d}t} = f(u(t, theta), theta)$ throughout the domain which we can do via lagrange multipliers. Introduce the auxiliary vector variable $ lambda$ . $$G(u(t, theta)) = int_{t_{0}}^{T}g(u(t, theta)) , mathrm{d}t+ int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{du}{ , mathrm{d}t}-f(u(t, theta), theta) right) , mathrm{d}t$$ Now, take $ frac{ partial}{ partial theta_i}$ to evaluate the iáµÊ° entry of the gradient. Apply the chain rule to find the derivative of the RHS. . $$ frac{ partial}{ partial theta_i}G(u(t, theta)) = int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}+ frac{ partial g}{ partial u} frac{ partial u}{ partial theta_i} right) , mathrm{d}t+ int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{d}{d theta_i} frac{ partial u}{ partial t}- left( frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial theta_i} right) right) , mathrm{d}t$$ extract the last integral from that expression, and split it into two. . $$ int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{d}{d theta_i} frac{ partial u}{ partial t}- left( frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial theta_i} right) right) , mathrm{d}t = int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{d}{d theta_i} frac{ partial u}{ partial t} right) , mathrm{d}t- int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial theta_i} right) , mathrm{d}t$$ For the first of those two expressions, apply integration by parts . $$ int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{d}{d theta_i} frac{ partial u}{ partial t} right) , mathrm{d}t = big[ lambda^{ intercal}(t) frac{ partial u}{ partial theta_i} big]_{t_{0}}^{T}- int_{t_{0}}^{T} left( frac{d lambda^{ intercal}(t)}{ , mathrm{d}t} frac{ partial u}{ partial theta_i} right) , mathrm{d}t$$ Now form the resultant expression . $$ begin{aligned} frac{d}{d theta_i}G(u(t, theta)) &amp; = int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}+ frac{ partial g}{ partial u} frac{ partial u}{ partial theta_i} right) , mathrm{d}t+ left[ big[ lambda^{ intercal}(t) frac{ partial u}{ partial theta_i} big]_{t_{0}}^{T}- int_{t_{0}}^{T} left( frac{d lambda^{ intercal}(t)}{ , mathrm{d}t} frac{ partial u}{ partial theta_i} right) , mathrm{d}t- int_{t_{0}}^{T} lambda^{ intercal}(t) left( frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial theta_i} right) , mathrm{d}t right] % &amp; = big[ lambda^{ intercal}(t) frac{ partial u}{ partial theta_i} big]_{t_{0}}^{T}+ int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}+ frac{ partial g}{ partial u} frac{ partial u}{ partial theta_i}- frac{d lambda^{ intercal}(t)}{ , mathrm{d}t} frac{ partial u}{ partial theta_i}- lambda^{ intercal}(t) left( frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial theta_i} right) right) , mathrm{d}t end{aligned}$$ Now here&#39;s the magic step. If we require: . $$ begin{aligned} frac{d lambda^{ intercal}(t)}{ , mathrm{d}t} &amp;=- frac{ partial f}{ partial u} lambda^{ intercal}(t)+ frac{ partial g}{ partial u}^{ intercal} % lambda(T) &amp;=0 end{aligned}$$ Then our expression simplifies . $$ begin{aligned} frac{d}{d theta_i}G(u(t, theta)) &amp;=- big[ lambda^{ intercal}(t) frac{ partial u}{ partial theta_i} big]_{t_{0}}^{T}+ int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}+ frac{ partial g}{ partial u} frac{ partial u}{ partial theta_i}- left(- frac{ partial f}{ partial u} lambda(t)+ frac{ partial g}{ partial u}^{ intercal} right) frac{ partial u}{ partial theta_i}- lambda^{ intercal}(t) left( frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial theta_i} right) right) , mathrm{d}t % &amp; =- big[0 cdot frac{ partial u}{ partial theta_i} Big|_T- lambda^{ intercal}(t_{0}) cdot frac{ partial u}{ partial theta_i} Big|_{t_0} big]+ int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}+ frac{ partial g}{ partial u} frac{ partial u}{ partial theta_i}+ frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i} lambda^{ intercal}(t)- frac{ partial g}{ partial u} frac{ partial u}{ partial theta_i}- lambda^{ intercal}(t) frac{ partial f}{ partial u} frac{ partial u}{ partial theta_i}- lambda^{ intercal}(t) frac{ partial f}{ partial theta_i} right) , mathrm{d}t % &amp; = int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}- lambda^{ intercal}(t) frac{ partial f}{ partial theta_i} right) , mathrm{d}t end{aligned}$$ The Adjoint Method expression for the gradient of the loss $G$ with respect to the parameters $ theta$ is $ frac{dG(u(t, theta))}{d theta_i} = int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}- lambda^{ intercal}(t) frac{ partial f}{ partial theta_i} right) , mathrm{d}t$. The savings over the forward sensitivities gradient expression results from the fact that $ lambda^{ intercal}(t)$ is the same for every $ theta_i$. The Adjoint Method expression contains three terms: . $ frac{ partial g}{ partial theta_i} = 0$ in our case, because our loss function $G(u(t, theta)) = sum_t ( frac{ mathrm{obs}(t) - u(t, theta)}{ sigma cdot sqrt{ mathrm{obs}(t)}})^2$ contains no explicit $ theta$ term, as it might if our loss included a regularization term. | $ frac{ partial f}{ partial theta_i}$ appeared in our forward sensitivity ODEs. It hasn&#39;t changed. | That leaves us with $ lambda^{ intercal}(t)$, which we need to compute. In order to cancel terms in our derivation, we had defined $ lambda^{ intercal}(t)$ via an ODE: $$ begin{aligned} frac{d lambda^{ intercal}(t)}{dt} &amp; =- frac{ partial f}{ partial u} lambda^{ intercal}(t)+ frac{ partial g}{ partial u}^{ intercal} % lambda(T) &amp; =0 end{aligned}$$ We can solve for $ lambda^{ intercal}(t)$ by integrating this &quot;adjoint ODE&quot;, however, since the boundary condition (the &quot;initial value&quot;) occurs at the last timepoint, we need to integrate this differential expression backwards in time. $$- frac{d lambda^{ intercal}(t)}{dt} = frac{ partial f}{ partial u} lambda^{ intercal}(t)- frac{ partial g}{ partial u}^{ intercal}$$ This adjoint ODE includes yet more terms: $ frac{ partial f}{ partial u}$ and $ frac{ partial g}{ partial u}^{ intercal}$. | $ frac{ partial f}{ partial u}$ is the Jacobian, also unchanged from our forward sensitivity ODEs. Resolving the entries of this matrix requires we have $u(t, theta)$. Thus in order to integrate the adjoint ODE we will need to have already integrated the system ODE. | Finally, $ frac{ partial g}{ partial u}^{ intercal}$ is the derivative of the instantaneous loss with respect to the state. Our instantaneous loss is zero everywhere except the timepoints we observe: $$ begin{aligned} g(u(t, theta)) &amp;= left( frac{ mathrm{obs}(t) - u(t, theta)}{ sigma cdot sqrt{ mathrm{obs}(t)}} right)^2 cdot sum_{t_i in mathrm{obs}} delta(t_i-i) frac{ partial}{ partial u}g(u(t, theta)) &amp;= begin{cases} frac{2( mathrm{obs}(t) - u(t, theta))}{ sigma cdot sqrt{ mathrm{obs}(t)}} &amp; t in mathrm{obs} 0 &amp; t notin mathrm{obs} end{cases} end{aligned}$$ | . The Adjoint Method for ODEs is thus: . Integrate the system ODE $ frac{du}{dt} = f(u( theta, t), theta)$, providing $u(t, theta)$. | Integrate the adjoint ODE $- frac{d lambda^{ intercal}(t)}{dt} = frac{ partial f}{ partial u} lambda^{ intercal}(t)- frac{ partial g}{ partial u}^{ intercal}$ in reverse from $ lambda(T) = 0$ to $t_0$, providing $ lambda(t)$. | Evaluate the integral $ frac{dG(u(t, theta))}{d theta_i} = int_{t_{0}}^{T} left( frac{ partial g}{ partial theta_i}- lambda^{ intercal}(t) frac{ partial f}{ partial theta_i} right)dt$ | Inspecting this procedure, we notice the integral in step 3 can be computed concurrently with step 2 for some time-savings: $ frac{dG(u(t, theta))}{d theta_i} = int_{T}^{t_{0}} left(- frac{ partial g}{ partial theta_i}+ lambda^{ intercal}(t) frac{ partial f}{ partial theta_i} right)dt$. . Let&#39;s implement it. . # define gradient_via_euler_adjoint_method, integrate_euler_adjoint() def gradient_of_loss_via_euler_adjoint_method(u_t, Î¸_t): Î»_t = integrate_euler_adjoint(u_t, Î¸_t) return Î»_t[grad_cols].iloc[0].to_dict(), Î»_t, u_t Î»_cols = [&#39;Î»_S&#39;,&#39;Î»_E&#39;,&#39;Î»_ES&#39;,&#39;Î»_P&#39;] grad_cols = [&#39;k_on&#39;,&#39;k_off&#39;,&#39;k_cat&#39;,&#39;k_uncat&#39;] def integrate_euler_adjoint(u, Î¸_t, dt=dt, steps=steps): k_on, k_off, k_cat, k_uncat, *args = Î¸_t.values() Î» = np.zeros((1, 4)) Î»_traj = [] grad = np.zeros((4, 1)) grad_traj = [] for t, (S, E, ES, P) in u[::-1].iterrows(): Î»_traj.append(Î»[0].copy()) grad_traj.append(grad.T[0].copy()) f_u = f_u_Jacobian(S, E, ES, P, k_on, k_off, k_cat, k_uncat) dÎ» = Î» @ f_u if t in observations.index: g_u_P = 2*(observations.loc[t] - u.loc[t, &#39;P&#39;]) dÎ» = dÎ» - np.array([[0, 0, 0, g_u_P]]) Î» += dÎ» * dt grad += np.array([np.dot(Î», f_Î¸(S, E, ES, P))[0] for f_Î¸ in [f_k_on, f_k_off, f_k_cat, f_k_uncat]]) * dt return pd.DataFrame(np.concatenate((Î»_traj, grad_traj), axis=1), columns=Î»_cols+grad_cols, index=u.index[::-1]).iloc[::-1] . # compute the first gradient of loss via our euler adjoint_method, from Î¸_0 Î¸_0[&#39;k_ms&#39;] = k_ms(Î¸_0) Î¸_0[&#39;k_mp&#39;] = k_mp(Î¸_0) start = time.process_time() grad_0, Î»_0, u_0 = gradient_of_loss_via_euler_adjoint_method(integrate_euler_full(Î¸_0), Î¸_0) euler_time = time.process_time() - start . # define plot_adjoint() def plot_adjoint(u, Î»): plt.rcParams[&#39;figure.figsize&#39;] = [12, 12] fig, axs = plt.subplots(3, 1, sharex=&#39;col&#39;) u.plot.line(ax=axs[0], color=color(u.columns)) traj_scipy_full.plot.line(ax=axs[0], lw=0.5, linestyle=&#39;--&#39;, legend=False, color=color(traj_scipy_full.columns)) observations.plot.line(ax=axs[0], marker=&#39;o&#39;, lw=0, color=color([&#39;P&#39;]), legend=False) Î»[Î»_cols].plot.line(ax=axs[1], color=color([c.split(&#39;Î»_&#39;)[1] for c in Î»_cols])) Î»[grad_cols].plot.line(ax=axs[2], color=color(grad_cols)) Î»[grad_cols].iloc[[0]].plot.line(ax=axs[2], marker=&#39;D&#39;, lw=0, legend=False, color=color(grad_cols)) for ax in axs: fig_style(ax) axs[1].set_ylabel(&#39;&#39;) axs[2].set_ylabel(&#39;&#39;) # plt.tight_layout() . plot_adjoint(u_0, Î»_0) . We can check that our gradient computation via the Adjoint Method is correct by comparing it to the Finite Differences approximation of the gradient: . def cosine_distance(a, b): return 1 - np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)) grad_0_fd = pd.Series(finite_differences(None, Î¸_0)).iloc[0:4] grad_0_adjoint_euler = pd.Series(grad_0) cosine_distance(grad_0_fd, grad_0_adjoint_euler) . 1.133973482914108e-05 . That&#39;s good news, however, . f&#39;A single step of the Adjoint Method with the naÃ¯ve python Euler method takes {round(euler_time, 2)}s&#39; . &#39;A single step of the Adjoint Method with the naÃ¯ve python Euler method takes 57.91s&#39; . As before, we would like to use scipy to evaluate the integrals needed to compute the gradient. This time we run into two new issues: . One reason the scipy integrate package&#39;s integrators are faster is because they use adaptive timestepping methods. In our Euler Methods, we set out timestep size dt at the outset, which remains constant throughout integration, meaning our resulting trajectory $u(t, theta)$ is really a vector of values of $u$ defined at uniformly-spaced values of $t$. Adaptive timestepping integrators yield trajectories supported on non-uniformly-spaced values of $t$, which we won&#39;t know prior to integration. If we integrate $ frac{du}{dt}$ with adaptive timestepping, and subsequently $- frac{d lambda}{dt}$, the resultant trajectories will not be supported on the same values of $t$. We can work around this issue by interpolating the trajectory of $u$ for values of $t$ between the $t$ the solution is defined at. . | Another issue arises from the fact that scipy&#39;s main integration method is not able to integrate differential equations with jump discontinuities of the sort we have in our Adjoint ODE with finite observations (as in the second plot above). Thankfully, Julia&#39;s feature-rich DifferentialEquations.jl library has bindings to python via diffeqpy, and does support this use case. . | . # import Julia&#39;s DifferentialEquations.jl python bindings # import julia # julia.install() # import diffeqpy # diffeqpy.install() from diffeqpy import de plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] . # define integrators to implement the Adjoint Method with Julia&#39;s DifferentialEquations.jl via diffeqpy def dy_julia(dy, y, p, t): # y ordered S,E,ES,P # p is ordered k_on,k_off,k_cat,k_uncat dy[0] = p[1] * y[2] - p[0] * y[1] * y[0] dy[1] = p[1] * y[2] - p[0] * y[1] * y[0] + p[2] * y[2] - p[3] * y[1] * y[3] dy[2] = p[0] * y[1] * y[0] - p[1] * y[2] - p[2] * y[2] + p[3] * y[1] * y[3] dy[3] = p[2] * y[2] - p[3] * y[1] * y[3] def integrate_julia_full(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps): y0 = list(initial_conditions.values()) t_span = (0, dt*steps) prob = ode.ODEProblem(dy_julia, y0, t_span, [*kinetic_params.values()]) try: sol = ode.solve(prob, ode.Tsit5(), tstops=list(observations.index)) return pd.DataFrame(sol.u, index=sol.t, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) except: return pd.DataFrame(columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) def dÎ»_julia(dÎ», Î», p, t): t = 0.5-t u_t = p[0](t) f_u = f_u_Jacobian(*u_t, *p[1:]) dÎ»[0:4] = Î»[0:4] @ f_u dÎ»[4:8] = np.array([np.dot(Î»[0:4], f_Î¸(*u_t)) for f_Î¸ in [f_k_on, f_k_off, f_k_cat, f_k_uncat]]).flatten() def g_u(integrator): t = np.round(0.5-integrator.t, 10) u_t = integrator.p[0](t) g_u_P = 2*(observations.loc[t] - u_t[3]) integrator.u[3] -= g_u_P def integrate_adjoint_julia(u_t_interpolant, Î¸_t, dt=dt, steps=steps): y0 = np.zeros(8) t_span = (0, dt*steps) prob = de.ODEProblem(dÎ»_julia, y0, t_span, [u_t_interpolant, *Î¸_t.values()]) try: sol = de.solve(prob, de.Tsit5(), tstops=list(observations.index), callback=de.PresetTimeCallback(list(observations.index), g_u)) return pd.DataFrame(sol.u, index=0.5-sol.t, columns=Î»_cols+grad_cols).iloc[::-1] except: return pd.DataFrame(columns=Î»_cols+grad_cols) def gradient_of_loss_via_julia_adjoint_method(u_t, Î¸_t): u_t_interpolant = scipy.interpolate.interp1d(u_t.index.values, u_t.values, kind=&#39;linear&#39;, axis=0, copy=False) Î»_t = integrate_adjoint_julia(u_t_interpolant, Î¸_t) return Î»_t[grad_cols].iloc[0].to_dict(), Î»_t, u_t . # test our Julia-based Adjoint Method Î¸_0 = {k:v for k,v in Î¸_0.items() if k in [&#39;k_on&#39;,&#39;k_off&#39;,&#39;k_cat&#39;,&#39;k_uncat&#39;]} start = time.process_time() grad_0_adjoint_julia, Î»_0_julia, u_0_julia = gradient_of_loss_via_julia_adjoint_method(integrate_julia_full(Î¸_0), Î¸_0) julia_time = time.process_time() - start plot_adjoint(u_0_julia, Î»_0_julia) . We recapitulate the results from our Euler method. . julia_adjoint_grad_0 = pd.Series(grad_0_julia) cosine_distance(grad_0_fd, grad_0_adjoint_julia) . 1.7326465229228205e-06 . f&#39;A single step of the Adjoint Method with julia takes {round(julia_time, 2)}s&#39; . &#39;A single step of the Adjoint Method with julia takes 1.07s&#39; . Now we can conceive of optimizing kinetic parameters with the adjoint method. . optimization_record = optimize_by_gradient_descent(Î¸_0, integrate=integrate_julia_full, gradient=gradient_of_loss_via_julia_adjoint_method) plot_optimization_trajectory(optimization_record) . In principle we could use the adjoint method to compute derivatives in log space as well, but we&#39;ll &quot;leave that as an exercise for the reader&quot;. . Further Reading . We have implemented the elementary techniques from the two families of approaches for ODE parameter inference. More advanced approaches from each family bear mentioning. . Advanced Bayesian Approaches . Bayesian approaches estimate a posterior distribution over the parameters to infer. . There are more practical and scalable variants of the inference by sampling approach. We implemented Markov-Chain Monte Carlo with a Metropolis-Hastings update criterion, which is hardly ever used in practice due to its simplemindedness. . Hamiltonian Monte Carlo is used in practice for many inference tasks and is a default in most probabilistic programming languages. I would recommend A Conceptual Introduction to Hamiltonian Monte Carlo. | Gibbs Sampling is another popular approach better suited in cases in which there are a greater number of parameters to infer. | Many MCMC variants have been proposed. Here is a visualization of some of them. | . | A newer approach to estimate the posterior distribution is not driven by sampling but instead on optimizing the parameters of a surrogate distribution. This approach is named Variational Inference. As an introduction I would recommend these lecture notes. Many VI variants have been proposed, but of particular note are SVI and ADVI. . | . Advanced Frequentist Approaches . The frequentist approach is to optimize our parameters against our Loss function. We implemented Gradient Descent with Momentum using gradients computed via Forward Sensitivities and the Adjoint Method. There are more practical and scalable variants of each of those parts. . There exist many algorithms which use gradients to descend on a loss surface. Beyond momentum we could have used any of the gradient-based opimtization procedures, often applied to optimize the parameters of Neural Networks (some of which are visualized here). Alternatively, for a few more evaluations of the ODE and the loss, we could have implemented a Line Search, which may have worked quite well on our relatively simple loss surface. | Beyond gradients, optimization can be dramatically accelerated by the evaluation of local second derivatives of the loss. Newton&#39;s method iteratively approximation the loss with a quadratic (a second-order taylor expansion) and steps directly to the bottom of the paraboloid at each iteration, yielding much greater convergence rates. When the number of parameters is large, and evaluation of the full Hessian (the 2â¿áµ derivative) at each iteration is intractable, pseudo-second-order optimization techniques such as BFGS (and the more practical L-BFGS) approximate the curvature of the loss surface from a sequence of gradients, and can work very well in practice. | Besides optimization techniques, the problem setting may admit alternate formulations of the optimization problem. Trajectory Optimization techniques may be brought to bear, particularly Multiple Shooting and Collocation methods. | . Implementations . This blog post is pedagogical. Proper implementations of these techniques should be used in practice. . The SciML ecosystem in Julia offers the best tooling I&#39;ve found for these problems. DiffEqFlux.jl offers implementations of many of the frequentist approaches, and Turing.jl offers implementations of the Bayesian approaches. Both rely on DifferentialEquations.jl. [Disclaimer: I have the privilege of working with the people who built this tooling.] | pyPESTO appears to be the best-maintained python toolchain for parameter estimation for biological systems models. There may be good python tooling I&#39;m not aware of (reach out!) especially originating from the Neural ODE literature, which has introduced the adjoint method to the Machine Learning community. | . Conclusions . Having learned to infer the parameters of ODE models, let&#39;s now take a step back and ask whether we have accomplished something meaningful. . Do we know the right model? . Let us not pretend to the world that we will eventually have predictive models of biology in the same way we have predictive models of airplanes. . â Jeremy Gunawardena . Were we to dispense a specific number of enzyme and substrate molecules into a fixed volume of continuously mixed solvent at a known temperature, pH, and ionic concentration, then the Law of Mass Action we assumed at the outset would hold, and the form of our differential equation model would apply. However, as Lucas Pelkmans writes &quot;solution chemistry does not apply to cells, in which everything is inhomogeneous, crowded, out-of-equilibrium, and unmixed.&quot; In this post, we explored techniques to fit the parameters of a chosen differential equation model, but determining the right model to begin with may be the greater challenge. For example, in the case of enzymatic catalysis, the effective concentration of enzyme and substrate may vary substantially inside a single cell, facilitated by phase separation (see The role of liquidâliquid phase separation in regulating enzyme activity and A new phase for enzyme kinetics). This fact is incompatible with the mass-action model we started out with, as our kinetic parameters would appear to be functions of the concentrations of various intracellular metabolites rather than constants. Enzyme kinetic parameters are already functions of the pH, salt, and temperature conditions of the reaction environment, which is likely the cause for the inconsistent estimates of enzymes&#39; kinetic parameters in databases such as BRENDA. Mathematical models from analytical chemistry may simply be inadequate to address the relevant phenomena in biology, which leads us to ask: what are the right models to describe these phenomena? . Further Reading: . Models in biology: âaccurate descriptions of our pathetic thinkingâ | Some lessons about models from Michaelis and Menten | . Is a wrong model useful? . All models are wrong, but some are useful. . â George Box . Putting aside the former question, in this work, our synthetic data were explicitly generated from the Mass Action rate equations, with fixed kinetic parameters, not a biological system. Even still, our inference techniques did not recover the original kinetic parameters. Instead, they found alternate kinetic parameters which predicted the observations nearly as well. When we visualized the loss surface, we discovered a region of parameter space able to predict the observed data. This phenomenon has been called &quot;Practical Identifiability&quot; (or lack thereof) and &quot;Model Sloppiness&quot;, and may be addressible by model order reduction and parameter space compression) techniques. However, we should ask ourselves why we are concerned that the parameters of a perfectly predictive model are not unique. This question gets at the heart of scientific modeling. Our goal is to arrive at a model in which the variables and parameters have a tight correspondence with the physical phenomena they represent. If we achieve that, then our model will be general, and accurately predict the outcomes of interventions not previously observed -- it will extrapolate. Furthermore, it will be interpretable, which makes it easier to design interventions. For the time being, highly predictive black-box models are missing the hallmarks of traditional scientific models. . A highly-predictive differential equation model with aphysical parameters is only marginally more useful than an overparameterized black-box model. In the future, we will aim for models composed of physically-salient variables and parameters. . Further Reading: . The &quot;Norvig - Chomsky debate&quot; | . Is a large model useful? . With four parameters I can fit an elephant, and with five I can make him wiggle his trunk. . â John von Neumann . We alluded to the long-term aspiration of mathematical biologists to model entire cells. Cells contain millions of chemical species, so a Chemical Reaction Network model of the sort we used here would entail millions of variables, millions of reaction equations, and millions more kinetic parameters. To a physicist, a model should reveal the essence of a system, and parameters are best restricted or avoided altogether. Would a million-parameter ODE model be any more interpretable or insightful than a million parameter neural network? . I believe that it would. A large physical model is an unusual thing today:models are mostly either physical, elegant, and concise ($f = ma$), or statistical, highly predictive, and overparameterized ($f = mathrm{NN}_{ theta}( cdot)$). I contend that large physical models provide many of the advantages of each of the former categories, namely, they may be highly predictive of complex phenomena yet interpretably so. The best example of a very large, very detailed, very useful model is Google Maps. The overwhelming quantity of data in Google&#39;s model of the world is useful in large part thanks to the exceptional interactive data visualization tool they provide to browse the model. The cell not only needs modeling, but representations and visualizations to explore and interrogate that model. . Is (math) modeling biology useful? . The best material model for a cat is another, or preferably the same cat. . â Arturo Rosenblueth and Norbert Wiener . If you speak to a biologist about a &quot;model&quot;, they may assume you mean a mouse, not a system of equations. The shared use of the same term is hardly coincidental:both approaches offer surrogates to the systems we seek to understand. We should ask ourselves why to invest in mathematical models of biological systems rather than biological models of those systems. In particular, experimentalists continue to develop techniques to precisely perturb and broadly measure biological systems. If we can perfectly control the inputs and measure the outputs of a biological model, why bother with a mathematical model? . This is a serious question in light of the relative advantages of biological models, which include: . Accuracy: we continue to discover new principles governing the behavior of biological matter, meaning that even a mathematical model consolidating the entirety our present understanding of cells would not be faithful to the real thing. Meanwhile, it is tautological that cells model themselves with perfect accuracy, even if the biochemical mechanisms mapping inputs to outputs are obscure. | Cost: cells &quot;compute&quot; their destinies with extremely little energy input, whereas large mathematical models require massive computing infrastructure and megawatts of power. | Parallelism: Experimentalists have developed technologies to interrogate biology at the level of the single cell, in multiplex. It is now possible to run millions of variants of an experiment simultaneously. That scale is not feasible in silico. | . Despite those, mathematical models confer their own advantages, which justify their pursuit: . Timeliness: In silico experiments are quick to set up, and yield answers on a timescale of minutes rather than weeks. | Interpretability: The entirety of the mechanism mapping inputs to outputs is transparent to us, by default. | Reproducibility: Biological models often behave in accordance with covariates we do not measure or control. We expect mathematical models running on deterministic computers to be perfectly reproducible. | Theory: A rich body of scholarship in optimal design &amp; control theory can be brought to bear against a mathematical model, as well as many other mathematical frameworks for the analysis of differential equations from the physical sciences. | Learning from mistakes: Insofar as mathematical models fail to predict the behavior of a system, they demonstrate the inaccuracies of their assumptions and ours. | . Ultiumately, we pursue mathematical models because unlike a mouse model or a statistical model, the mathematical model alone represents understanding of the phenomenon. We apprehend the world as causes and effects, which we have formalized in the language of Differential Equations. True human understanding of the cell will be reached once we have a complete mathematical model. . Further Reading: . Theory in Biology: Figure 1 or Figure 7? | . Related Work . Fabian FrÃ¶hlich: Scalable Parameter Estimation for Genome-Scale Biochemical Reaction Networks | Christopher R. Myers: Zen and the Art of Parameter Estimation in Systems Biology | Demetri Pananos: Gradient Descent for ODEs | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2021/06/23/Enzyme-Kinetic-Parameter-Inference.html",
            "relUrl": "/2021/06/23/Enzyme-Kinetic-Parameter-Inference.html",
            "date": " â¢ Jun 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Intuition for Second Order Partial Derivatives and the Hessian Matrix",
            "content": "# imports # using Plots; plotlyjs() using PlotlyJS # from IPython.display import HTML # HTML(fig.to_html()) # where fig = plotly.plot(...) . Unable to load WebIO. Please make sure WebIO works for your Jupyter client. For troubleshooting, please see the WebIO/IJulia documentation. . The Hessian matrix appears in the optimization literature, but the intuition for how the Hessian and its inverse transform vectors is opaque to me. Let&#39;s review second order partial derivatives, and then try to build intuition for the Hessian matrix. . For the purpose of this intuition-building exercise, we&#39;ll work with functions $ Reals^2 mapsto Reals^1$. I&#39;ll also use partial derivative notations $ frac{ partial}{ partial y} f(x, y) = frac{ partial f}{ partial y} = f_y$ interchangeably. . 1. Partial Derivatives . Take the $ Reals^2 mapsto Reals^1$ function $f(x, y) = x^2 + 2y^2$. . A partial derivative is the change in an &quot;output&quot; variable (in our case, $f$) with respect to infinitesimal changes in an &quot;input&quot; variable (in our case, $x$ or $y$). For example, $ frac{ partial}{ partial y} f(x, y) = 4y$, which is to say, for any point in the domain, moving infinitsimally in the y direction changes f propotional to 4 times the y coordinate of the starting point point. . f(x, y) = x^2 + 2y^2 x = 6 xlim=[-10, x] ylim=[-10, 10] xs = LinRange(xlim..., 101) ys = LinRange(ylim..., 101) zs = [f(x, y) for x in xs, y in ys] y = 4 dy = 4 f_y(y) = 4y . f_y (generic function with 1 method) . # built interactive plot traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) push!(traces, PlotlyJS.surface(x=[x, x+0.001], y=ylim, z=[[maximum(zs), minimum(zs)] [maximum(zs), minimum(zs)]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.2)) push!(traces, PlotlyJS.scatter3d(x=fill(x, size(ys)), y=ys, z=[x^2 + 2y^2 for y in ys], showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;red&quot;, width=2))) for y in ys[1:5:end] push!(traces, PlotlyJS.scatter3d(x=fill(x, 2),y=[y-dy, y+dy], z=[f(x,y)-f_y(y)*dy, f(x,y)+f_y(y)*dy], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) end scene = attr( xaxis = attr(range=[-10,10]), yaxis = attr(range=[-10,10]), zaxis = attr(range=[-50,300]), aspectratio = attr(x=1, y=1, z=1) ) layout = Layout( sliders=[attr( steps=[ attr( label=round(y, digits=2), method=&quot;update&quot;, args=[attr(visible=[fill(true, 3); fill(false, i-1); true; fill(false, 101-i)])] ) for (i, y) in enumerate(ys[1:5:end]) ], active = y, currentvalue_prefix=&quot;x = 6, y = &quot;, # pad_t=40 )], scene = scene, ) p = PlotlyJS.plot(traces, layout) . We can plot the function $f_y$ for every starting point: . # plot partial derivative of f with respect to y, f_y traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) push!(traces, PlotlyJS.surface(x=ylim, y=ylim, z=[[0, 0] [0, 0]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.3)) push!(traces, PlotlyJS.surface(x=xs, y=ys, z=[f_y(y) for x in xs, y in ys], showscale=false)) plot(traces, Layout(scene=scene)) . We can do the exact same exercise with $f_x$: . f_x(x) = 2x . f_x (generic function with 1 method) . traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) push!(traces, PlotlyJS.surface(x=ylim, y=ylim, z=[[0, 0] [0, 0]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.3)) push!(traces, PlotlyJS.surface(x=xs, y=ys, z=[f_x(x) for x in xs, y in ys], showscale=false)) plot(traces, Layout(scene=scene)) . So the way the second order partial derivative is defined is as a composition, e.g. $f_{xx} = frac{ partial}{ partial x} left( frac{ partial}{ partial x} left( f(x, y) right) right) $. As second derivatives do, it captures the [change in the [change in the [output variable]]] with respect to infinitesimal changes in the input variable. This notion coincides with the curvature of the function: a positive second derivative at a particular point indicates that the output variable is concave up at a that point, and a negative second derivative indicates the output variable is concave down at that point. . In the case of the function we&#39;ve chosen, $f_{xx} = 2$ and $f_{yy} = 4$ which informs us that $f$ is concave up everywhere in the domain, which makes sense from looking at the plot. . However, we&#39;ve omitted the &quot;mixed&quot; partial derivatives here: $f_{xy}$ and $f_{yx}$. We can compute them to both be zero for this particular function. What does that tell us? . # plot partial derivative of f with respect to y, f_y traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=ylim, y=ylim, z=[[0, 0] [0, 0]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.3)) push!(traces, PlotlyJS.surface(x=xs, y=ys, z=[f_y(y) for x in xs, y in ys], showscale=false)) push!(traces, PlotlyJS.scatter3d(x=[-7, 4], y=[5, 5], z=[f_y(5), f_y(5)], showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) p = plot(traces, Layout(scene=scene)) . It&#39;s clear from the graph above that infinitesimal changes in $x$ do not influence the value of $f_y$. Interpreting $f_{yx} = frac{ partial}{ partial x} left( frac{ partial}{ partial y} left( f(x, y) right) right)$ as $ frac{ partial}{ partial x} f_y $, it&#39;s clear $f_{yx} = 0$. But how can we interpret that in terms of the original function f? . # built interactive plot traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) for x in xs[1:5:end] push!(traces, PlotlyJS.surface(x=[x, x+0.001], y=ylim, z=[[maximum(zs), minimum(zs)] [maximum(zs), minimum(zs)]], visible=false, showscale=false, colorscale=&quot;Greys&quot;, opacity=0.2)) push!(traces, PlotlyJS.scatter3d(x=fill(x, size(ys)), y=ys, z=[x^2 + 2y^2 for y in ys], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;red&quot;, width=2))) push!(traces, PlotlyJS.scatter3d(x=fill(x, 2),y=[y-dy, y+dy], z=[f(x,y)-f_y(y)*dy, f(x,y)+f_y(y)*dy], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) end layout = Layout( sliders=[attr( steps=[ attr( label=round(x, digits=2), method=&quot;update&quot;, args=[attr(visible=[fill(true, 1); fill(false, 3*(i-1)); fill(true, 3); fill(false, 3*(101-i))])] ) for (i, x) in enumerate(xs[1:5:end]) ], active = x, currentvalue_prefix=&quot;x = 6, y = &quot;, # pad_t=40 )], scene = scene, ) p = PlotlyJS.plot(traces, layout) . Here is where we find the intuition. The mixed second order partial derivatives tell us how the slope along one coordinate axis changes as we move infinitesimally along an orthogonal coordinate axis. In the function we&#39;ve chosen, the slice of the graph at any x coordinate is the same parabola (just vertically offset by $x^2$) and thus has the same slope for any y. The mixed second order partial derivatives could thus be said to relay information about the &quot;pucker&quot; of the graph of the function (I made that name up) which is the concavity of the graph with respect to two coordinate axes. . In order to see this more clearly, let&#39;s add a mixed term to our function, to produce non-zero mixed second order partial derivatives: . g(x, y) = x^2 + 2y^2 - x*y zs = [g(x, y) for x in xs, y in ys] g_x(x, y) = 2x - y g_y(x, y) = 4y - x . g_y (generic function with 1 method) . # built interactive plot traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) for x in xs push!(traces, PlotlyJS.surface(x=[x, x+0.001], y=ylim, z=[[maximum(zs), minimum(zs)] [maximum(zs), minimum(zs)]], visible=false, showscale=false, colorscale=&quot;Greys&quot;, opacity=0.2)) push!(traces, PlotlyJS.scatter3d(x=fill(x, size(ys)), y=ys, z=[g(x, y) for y in ys], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;red&quot;, width=2))) push!(traces, PlotlyJS.scatter3d(x=fill(x, 2),y=[y-dy, y+dy], z=[g(x,y)-g_y(x, y)*dy, g(x,y)+g_y(x, y)*dy], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) end layout = Layout( sliders=[attr( steps=[ attr( label=round(x, digits=2), method=&quot;update&quot;, args=[attr(visible=[fill(true, 1); fill(false, 3*(i-1)); fill(true, 3); fill(false, 3*(101-i))])] ) for (i, x) in enumerate(xs) ], active = x, currentvalue_prefix=&quot;x = 6, y = &quot;, # pad_t=40 )], scene = scene, ) p = PlotlyJS.plot(traces, layout) . The slope along the y coordinate changes as we vary x. This function is &quot;puckered&quot; up. . Note: For twice-continuously differentiable functions, $f_{xy}$ = $f_{yx}$. This is called the symmetry property of second derivatives. And leads to a symmetric Hessian matrix. We&#39;ll assume that property of our functions throughout. | . 2. Hessian Matrix . Having now built intuition for each of the entries in the Hessian Matrix, we can begin to build intuition for the matrix itself: . $$ bold{H}_f = begin{bmatrix} frac{ partial^2 f}{ partial x^2} &amp; frac{ partial^2 f}{ partial x partial y} frac{ partial^2 f}{ partial y partial x} &amp; frac{ partial^2 f}{ partial y^2} end{bmatrix} = begin{bmatrix} f_{xx} &amp; f_{yx} f_{xy} &amp; f_{yy} end{bmatrix}$$In some sense, the Hessian is a matrix-valued function we can evaluate for any point $ bold{v_0} = begin{bmatrix} x_0 y_0 end{bmatrix}$ in the domain of $f$: . $$ bold{H}_f( bold{v_0}) = begin{bmatrix} f_{xx}( bold{v_0}) &amp; f_{yx}( bold{v_0}) f_{xy}( bold{v_0}) &amp; f_{yy}( bold{v_0}) end{bmatrix}$$The Hessian shows up in the quadratic approximation (second order Taylor expansion) of multivariate functions around a particular point $ bold{v_0}$: . $$ Q_f( bold{v_0}) = color{green} f( bold{v_0}) color{black} + color{blue} nabla_f( bold{v_0}) cdot ( bold{v} - bold{v_0}) color{black} + color{indigo} {1 over 2}( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v} - bold{v_0})$$ . And we might ask: how does this matrix scale $( bold{v} - bold{v_0})$? Let&#39;s try to build some intuition: . Suppose we multiply some vector against that Hessian Matrix: . $$ begin{bmatrix} f_{xx}( bold{v_0}) &amp; f_{yx}( bold{v_0}) f_{xy}( bold{v_0}) &amp; f_{yy}( bold{v_0}) end{bmatrix} cdot begin{bmatrix} x y end{bmatrix} = begin{bmatrix} f_{xx} cdot x + f_{xy} cdot y f_{yy} cdot y + f_{yx} cdot x end{bmatrix} = $$$$ begin{bmatrix} ( text{the rate of change of the slope in the x direction as you move in the x direction} ) ( text{a distance in the x direction} ) + ( text{the rate of change of the slope in the x direction as you move in the y direction} ) ( text{a distance in the y direction} ) ( text{the rate of change of the slope in the y direction as you move in the y direction} ) ( text{a distance in the y direction} ) + ( text{the rate of change of the slope in the y direction as you move in the x direction} ) ( text{a distance in the x direction} ) end{bmatrix} = $$$$ begin{bmatrix} text{the approximate slope in the x direction at the distance x from }x_0 text{the approximate slope in the y direction at the distance y from }y_0 end{bmatrix} = $$ $$ text{what a second-order approximation of } f text{ suggests the gradient is at } bold{v_0} + begin{bmatrix} x y end{bmatrix} $$ . However, let&#39;s note that the quadratic approximation of $f$ includes as quadratic form $( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v} - bold{v_0})$, not just a matrix-vector product. Applying this second product operation results in a scalar, which is the change in the value of the function as a result of extrapolating the (second-order) curvature of the function at $ bold{v_0}$ out towards $ bold{v} - bold{v_0}$. . 3. Newton Method . Newton&#39;s method for optimization is an iteration of . forming the quadratic approximation $Q_f( bold{v_0})$ of a function around the current point. | jumping to the min/max of the paraboloid approximation for the next iteration. | . This requires us to identify the min/max of the second-order approximation of $f$. Recall, the quadratic approximation (second order taylor expansion) of $f$ at a particular point $ bold{v_0}$ is . $$ Q_f( bold{v_0}) = color{green} f( bold{v_0}) color{black} + color{blue} nabla_f( bold{v_0}) cdot ( bold{v} - bold{v_0}) color{black} + color{indigo} {1 over 2}( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v} - bold{v_0})$$ . Searching for the extrema of this approximation, we can take $ nabla left[ Q_f(v_0) right] = 0$. . $ nabla left[ color{green} f( bold{v_0}) color{black} right] = 0$ | $ nabla left[ color{blue} nabla_f( bold{v_0}) cdot ( bold{v} - bold{v_0}) color{black} right] = nabla_f( bold{v_0})$ | $ nabla left[ color{indigo} {1 over 2}( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v}- bold{v_0}) color{black} right] = {1 over 2} left( left[ bold{H}_f( bold{v_0}) right] bold{v}+ left[ bold{H}_f( bold{v_0}) right]^ intercal right) bold{v}$ | . We can simplify ${1 over 2} left( left[ bold{H}_f( bold{v_0}) right] bold{v}+ left[ bold{H}_f( bold{v_0}) right]^ intercal right) bold{v}$ further, because $ left[ bold{H}_f( bold{v_0}) color{black} right] = left[ bold{H}_f( bold{v_0}) right]^ intercal$, because of the symmetry property of second derivatives, so this expression simplifies to ${1 over 2} left( 2 left[ bold{H}_f( bold{v_0}) right] right) bold{v} = left[ bold{H}_f( bold{v_0}) right] bold{v}$. . So all together, $ nabla left[ Q_f( bold{v_0}) right] = color{blue} nabla_f( bold{v_0}) color{black} + color{indigo} left[ bold{H}_f( bold{v_0}) right] bold{v}$. . Well, $ nabla_f( bold{v_0}) + left[ bold{H}_f( bold{v_0}) right] bold{v} = 0$ when: . $$ begin{aligned} left[ bold{H}_f( bold{v_0}) right] bold{v} &amp;= - nabla_f( bold{v_0}) bold{v} &amp;= - left[ bold{H}_f( bold{v_0}) right]^{-1} cdot nabla_f( bold{v_0}) end{aligned} $$So $ bold{v}$ is the vector which denotes the extremum of the quadratic approximation of $f$ around $ bold{v_0}$. The familiar Newton method expression pops out of our above derivation: . $$ bold{v_{k+1}} = bold{v_k} - left[ bold{H}_f( bold{v_k}) right]^{-1} nabla_f( bold{v_k})$$ . And now we know that this expression which seemed to come out of nowhere $ left[ bold{H}_f( bold{v_k}) right]^{-1} nabla_f( bold{v_k})$ including an inverse Hessian (!!) is really just the $ bold{v}$ which solves $ nabla left[ Q_f(v_0) right] = 0$. . References . Khan Academy: Second-Order Partial Derivatives | Walter Schreiner&#39;s course notes: intuition for second order partial derivatives | Khan Academy: The Hessian | Khan Academy: Quadratic Approximations | Kris Hauser&#39;s course notes: derivation of Newton&#39;s Method (especially page 4) | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2021/05/18/Hessian-Intuition.html",
            "relUrl": "/2021/05/18/Hessian-Intuition.html",
            "date": " â¢ May 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Orthogonal Functions",
            "content": "# imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 Ï = np.pi exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt . Fourier Basis . grid = 200 domain = [0, 2*Ï] dx = (domain[1]-domain[0])/grid grid = np.linspace(*domain, grid) def fourier(k, x): return sin(k*x)+cos(k*x) . n = 5 basis = pd.DataFrame({k: fourier(k, grid) for k in range(1,n)}, index=grid) ax = basis.plot.line(lw=0.4, xlim=domain) ax.axhline(0, c=&#39;black&#39;, lw=&#39;0.3&#39;) . &lt;matplotlib.lines.Line2D at 0x136a4e890&gt; . from scipy import integrate def compare_two(i, j): product = pd.Series(basis[i]*basis[j], name=&#39;product&#39;) product = pd.DataFrame([basis[i], basis[j], product]).T ax = product.plot.line(lw=0.5, color=[&#39;red&#39;, &#39;blue&#39;, &#39;purple&#39;]) ax.fill_between(grid, product[&#39;product&#39;], alpha=0.1) return integrate.trapz(product[&#39;product&#39;], x=product.index) . print(&#39;integral =&#39;, np.round(compare_two(3,4), 4)) . integral = -0.0 . &quot;fourier modes as eigenfunctions of the derivative operator&quot; What? . Polynomial Bases .",
            "url": "https://alexlenail.me/back_of_my_envelope/2020/12/04/orthogonal-functions.html",
            "relUrl": "/2020/12/04/orthogonal-functions.html",
            "date": " â¢ Dec 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "",
          "url": "https://alexlenail.me/back_of_my_envelope/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Old Blog",
          "content": "",
          "url": "https://alexlenail.me/back_of_my_envelope/old/",
          "relUrl": "/old/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page7": {
          "title": "",
          "content": "{â/about/â:âhttp://alexlenail.meâ,â/old/â:âhttps://alexlenail.medium.com/â} .",
          "url": "https://alexlenail.me/back_of_my_envelope/redirects.json",
          "relUrl": "/redirects.json",
          "date": ""
      }
      
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ âsitemap.xmlâ | absolute_url }} | .",
          "url": "https://alexlenail.me/back_of_my_envelope/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}